{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8da33c-eeb6-4d72-a0fc-89f9fb6a19fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# RAG System Evaluation\n",
    "    \n",
    "This notebook is a follow up from the previous notebook in which we explored the overall evaluation approach and a RAG system's overall accuracy.\n",
    "\n",
    "This notebook we will take a closer look at specific RAG metrics and explore how different components and configurations can impact overall accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47020c-9266-4740-99c2-0d330b90d460",
   "metadata": {},
   "source": [
    "## Solution architecture\n",
    "<img src=\"https://d3q8adh3y5sxpk.cloudfront.net/meetingrecordings/modelevaluation/architecture.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">\n",
    "\n",
    "From the solution architecture, we will experiment with the below RAG components and evaluate the impact on several metric's relevant for RAG.\n",
    "\n",
    "- 1) Text Splitter: TokenTextSplitter vs CharacterTextSplitter\n",
    "- 2) Embedding model: amazon.titan-embed-text-v1 vs cohere.embed-multilingual-v3 \n",
    "- 3) Retriever: OpenSearch VectoreStoreRetriever search types “similarity” vs “mmr”\n",
    "- 4) Prompt Template: For each LLM we evaluate two different prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4514e6",
   "metadata": {},
   "source": [
    "## RAG evaluation metrics\n",
    "\n",
    "This notebook focuses on the following metrics:\n",
    "-  a. context_precision: evaluates whether all of the ground-truth relevant items are present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. Values range from 0 to 1, higher scores indicate better precision.\n",
    "-  b. faithfulness: measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. Values range from 0 to 1, with higher values indicating better performance.  \n",
    "-  c. context_recall: measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. Values range from 0 to 1, with higher values indicating better performance.\n",
    "-  d. answer_relevancy: focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information. This metric is computed using the question and the answer, with values ranging between 0 and 1, where higher scores indicate better relevancy.\n",
    "-  e. answer_correctness¶: the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. *TO DO: not implemented here yet\n",
    "-  f. answer_similarity: Scores the semantic similarity of ground truth with generated answer. *TO DO: not implemented here yet\n",
    "\n",
    "TO DO: \n",
    "1) LlamaIndex: \n",
    "-  a. Faithfulness: measure if the response from a query engine matches any source nodes\n",
    "-  b. Relevancy: measure if the response and source nodes match the query\n",
    "-  c. Correctness: assess the relevance and correctness of a generated answer against a reference answer\n",
    "-  d. Semantic Similarity: evaluates the quality of a question answering system via semantic similarity\n",
    "\n",
    "2) mlflow metrics: https://mlflow.org/docs/latest/python_api/mlflow.metrics.html\n",
    "\n",
    "\n",
    "Further information on RAG evaluation metrics can be found at https://docs.ragas.io/en/latest/concepts/metrics/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b3b3a",
   "metadata": {},
   "source": [
    "Following the metric-driven RAG development approach as described in https://blog.worldline.tech/2024/01/12/metric-driven-rag-development.html, we will initially focus on faithfulness, and then on context recall and context precision to derive at an optimal informaction extraction solution.\n",
    "\n",
    "<img src=\"./images/metrics_and_stage_long.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">\n",
    "\n",
    "<img src=\"./images/metrics_and_stage_long_II.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea6dd86c-115c-49b9-b112-548edf312d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing ./ragas-aws-1.0.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting awscli==1.32.19 (from -r requirements.txt (line 1))\n",
      "  Downloading awscli-1.32.19-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting boto3==1.34.19 (from -r requirements.txt (line 2))\n",
      "  Downloading boto3-1.34.19-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore==1.34.19 (from -r requirements.txt (line 3))\n",
      "  Downloading botocore-1.34.19-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting langchain==0.1.0 (from -r requirements.txt (line 4))\n",
      "  Downloading langchain-0.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langsmith==0.0.83 (from -r requirements.txt (line 5))\n",
      "  Downloading langsmith-0.0.83-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nltk<4.0.0 (from -r requirements.txt (line 6))\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting opensearch-py==2.4.2 (from -r requirements.txt (line 7))\n",
      "  Downloading opensearch_py-2.4.2-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting plotly==5.9.0 (from -r requirements.txt (line 8))\n",
      "  Downloading plotly-5.9.0-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pypdf==3.17.4 (from -r requirements.txt (line 9))\n",
      "  Downloading pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 10))\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting requests-aws4auth==1.2.3 (from -r requirements.txt (line 11))\n",
      "  Downloading requests_aws4auth-1.2.3-py2.py3-none-any.whl (24 kB)\n",
      "Collecting tiktoken==0.5.2 (from -r requirements.txt (line 12))\n",
      "  Downloading tiktoken-0.5.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting xmltodict==0.13.0 (from -r requirements.txt (line 13))\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting sagemaker==2.203.1 (from -r requirements.txt (line 14))\n",
      "  Downloading sagemaker-2.203.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting matplotlib==3.8.2 (from -r requirements.txt (line 15))\n",
      "  Using cached matplotlib-3.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting sympy==1.12 (from -r requirements.txt (line 16))\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting nbformat==5.9.2 (from -r requirements.txt (line 17))\n",
      "  Using cached nbformat-5.9.2-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting nest-asyncio (from -r requirements.txt (line 18))\n",
      "  Using cached nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting ipywidgets (from -r requirements.txt (line 20))\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pandas==2.1.3 (from -r requirements.txt (line 21))\n",
      "  Using cached pandas-2.1.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting fmeval==0.3.0 (from -r requirements.txt (line 22))\n",
      "  Using cached fmeval-0.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index (from -r requirements.txt (line 24))\n",
      "  Downloading llama_index-0.9.45.post1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting langchainhub (from -r requirements.txt (line 25))\n",
      "  Downloading langchainhub-0.1.14-py3-none-any.whl.metadata (478 bytes)\n",
      "Collecting pysbd (from -r requirements.txt (line 27))\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from -r requirements.txt (line 28))\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting torch (from -r requirements.txt (line 29))\n",
      "  Using cached torch-2.2.0-cp311-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 30))\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "\u001b[31mERROR: Cannot install langchain==0.1.0 and langchain==0.1.5 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested langchain==0.1.0\n",
      "    The user requested langchain==0.1.5\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "%pip install --force-reinstall -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40e9545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append your RagasBedrock package path\n",
    "import sys\n",
    "sys.path.append(\"./ragas/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba52a1c8-3b25-4bfa-bbff-5bbbd5de31c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# restart kernel to ensure proper version of libraries is loaded\n",
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b6646c5-8ccf-4e0b-8ea8-0501c379d551",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mawscli                    1.32.19\n",
      "boto3                     1.34.36\n",
      "botocore                  1.34.36\n",
      "langchain                 0.1.5\n",
      "langchain-community       0.0.17\n",
      "langchain-core            0.1.18\n",
      "langchain-openai          0.0.5\n",
      "langchainhub              0.1.14\n",
      "mlflow                    2.10.0\n",
      "mypy-boto3-bedrock        1.34.0\n",
      "nest-asyncio              1.6.0\n",
      "nltk                      3.8.1\n",
      "opensearch-py             2.4.2\n",
      "plotly                    5.9.0\n",
      "pypdf                     3.17.4\n",
      "python-dotenv             1.0.0\n",
      "requests-aws4auth         1.2.3\n",
      "sagemaker                 2.207.1\n",
      "tiktoken                  0.5.2\n",
      "xmltodict                 0.13.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~rllib3 (/Users/huthmac/.pyenv/versions/3.11.7/envs/rag-eval/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E \"awscli|boto3|botocore|langchain|mlflow|plotly|tiktoken|nltk|python-dotenv|xmltodict|requests-aws4auth|pypdf|opensearch-py|sagemaker|nest-asyncio\"\n",
    "# also review requirements.txt for reference if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e938c8c-5e0b-483c-9efd-84c7c9d4e3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load environment variables \n",
    "import boto3\n",
    "import os\n",
    "import botocore\n",
    "from botocore.config import Config\n",
    "import langchain\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from typing import Dict\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# loading environment variables that are stored in local file dev.env\n",
    "load_dotenv(find_dotenv('dev-mlflow.env'),override=True)\n",
    "\n",
    "\n",
    "\n",
    "os.environ['OPENSEARCH_COLLECTION'] = os.getenv('OPENSEARCH_COLLECTION')\n",
    "os.environ['AWS_ACCESS_KEY'] = os.getenv('AWS_ACCESS_KEY')\n",
    "os.environ['AWS_SECRET_TOKEN'] = os.getenv('AWS_SECRET_TOKEN')\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['MLFLOW_TRACKING_URI'] = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "\n",
    "\n",
    "# Initialize mlflow client\n",
    "mlflow_client = MlflowClient(tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\n",
    "\n",
    "# Initialize Bedrock runtime\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 8\n",
    "   }\n",
    ")\n",
    "bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        config=config\n",
    ")\n",
    "\n",
    "# Initialize sagemaker session\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd9a12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Experiment: artifact_location='s3://mlflow-artifacts-026459568683/5', creation_time=1707417147815, experiment_id='5', last_update_time=1707417147815, lifecycle_stage='active', name='RAG_system_accuracy_652', tags={'mlflow.note.content': 'RAG system evaluation project.',\n",
      " 'project_name': 'rag-eval',\n",
      " 'source': 'Amazon 10k',\n",
      " 'team': 'aws-ai-ml-analytics',\n",
      " 'use_case': 'information extraction'}>\n"
     ]
    }
   ],
   "source": [
    "# Create a new mlflow experiment\n",
    "import random\n",
    "experiment_description = (\n",
    "    \"RAG system evaluation project.\"\n",
    ")\n",
    "\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"rag-eval\",\n",
    "    \"use_case\": \"information extraction\",\n",
    "    \"team\": \"aws-ai-ml-analytics\",\n",
    "    \"source\": \"Amazon 10k\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "random_identifier = random.randrange(100, 1000, 3)\n",
    "experiment_name = f\"RAG_system_accuracy_{random_identifier}\"\n",
    "llm_experiment = mlflow_client.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "\n",
    "# Use search_experiments() to search on the project_name tag key\n",
    "rag_experiment = mlflow_client.search_experiments(\n",
    "    filter_string=\"tags.`project_name` = 'rag-eval'\"\n",
    ")\n",
    "\n",
    "print(rag_experiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9f1edb6-2102-48a2-bede-ffabd3caa462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huthmac/.pyenv/versions/3.11.7/lib/python3.11/json/decoder.py:353: RuntimeWarning: coroutine 'FaithfulnessEvaluator.aevaluate' was never awaited\n",
      "  obj, end = self.scan_once(s, idx)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize LLMs (Claude-V2, Cohere, LLama2)\n",
    "\n",
    "## 1a. Initialize Claude-v2\n",
    "llm01_inference_modifier = {\n",
    "    \"max_tokens_to_sample\": 545,\n",
    "    \"temperature\": 0,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "LLM_01_NAME= \"anthropic.claude-v2\"\n",
    "llm01 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_01_NAME,\n",
    "    model_kwargs=llm01_inference_modifier\n",
    ")\n",
    "\n",
    "## 1b. Initialize Cohere Command\n",
    "llm02_inference_modifier = { \n",
    "    \"max_tokens\": 545,\n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_02_NAME= \"cohere.command-text-v14\"\n",
    "llm02 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_02_NAME,\n",
    "    model_kwargs=llm02_inference_modifier\n",
    ")\n",
    "\n",
    "## 1c. Initialize Llama\n",
    "llm03_inference_modifier = { \n",
    "    \"max_gen_len\": 545,\n",
    "    \"top_p\": 0.9, \n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_03_NAME= \"meta.llama2-13b-chat-v1\"\n",
    "llm03 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_03_NAME,\n",
    "    model_kwargs=llm03_inference_modifier\n",
    ")\n",
    "\n",
    "llms = [\n",
    "    llm01,\n",
    "    llm02,\n",
    "    llm03\n",
    "]\n",
    "\n",
    "## 1d. Initialize eval llm\n",
    "inference_modifier = { \n",
    "    \"max_gen_len\": 545,\n",
    "    \"top_p\": 0.9, \n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_EVAL_NAME= \"meta.llama2-70b-chat-v1\"\n",
    "langchain_eval_llm = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_EVAL_NAME,\n",
    "    model_kwargs=inference_modifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60611855-58f1-4568-b250-88ed113262e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"Who is Amazon's Senior Vice President and General Counsel?\", 'context': 'Available Information\\nOur investor relations website is amazon.com/ir and we encourage investors to use it as a way of easily finding information about us. We promptly make available on this website, free of charge, the reports that we file or furnish with the Securities and Exchange Commission (â\\x80\\x9cSECâ\\x80\\x9d), corporate governance information (including our Code of Business Conduct and Ethics), and select press releases.\\nExecutive Officers and Directors\\nThe following tables set forth certain information regarding our Executive Officers and Directors as of January 25, 2023:\\nInformation About Our Executive Officers\\nName Age Position\\nJeffrey P. Bezos. Mr. Bezos founded Amazon.com in 1994 and has served as Executive Chair since July 2021. He has served as Chair of the Board since 1994 and served as Chief Executive Officer from May 1996 until July 2021, and as President from 1994 until June 1999 and again from October 2000 to July 2021.\\nAndrew R. Jassy. Mr. Jassy has served as President and Chief Executive Officer since July 2021, CEO Amazon Web Services from April 2016 until July 2021, and Senior Vice President, Amazon Web Services, from April 2006 until April 2016.\\nDouglas J. Herrington. Mr. Herrington has served as CEO Worldwide Amazon Stores since July 2022, Senior Vice President, North America Consumer from January 2015 to July 2022, and Senior Vice President, Consumables from May 2014 to December 2014.\\nBrian T. Olsavsky. Mr. Olsavsky has served as Senior Vice President and Chief Financial Officer since June 2015, Vice President, Finance for the Global Consumer Business from December 2011 to June 2015, and numerous financial leadership roles across Amazon with global responsibility since April 2002.\\nShelley L. Reynolds. Ms. Reynolds has served as Vice President, Worldwide Controller, and Principal Accounting Officer since April 2007.\\nAdam N. Selipsky. Mr. Selipsky has served as CEO Amazon Web Services since July 2021, Senior Vice President, Amazon Web Services from May 2021 until July 2021, President and CEO of Tableau Software from September 2016 until May 2021, and Vice President, Marketing, Sales and Support of Amazon Web Services from May 2005 to September 2016.\\nDavid A. Zapolsky. Mr. Zapolsky has served as Senior Vice President, General Counsel, and Secretary since May 2014, Vice President, General Counsel, and Secretary from September 2012 to May 2014, and as Vice President and Associate General Counsel for Litigation and Regulatory matters from April 2002 until September 2012.\\n5\\nJeffrey P. Bezos\\nAndrew R. Jassy\\nDouglas J. Herrington\\nBrian T. Olsavsky\\nShelley L. Reynolds\\nAdam N. Selipsky\\nDavid A. Zapolsky\\n59\\n55\\n56\\n59\\n58\\n56\\n59\\nExecutive Chair\\nPresident and Chief Executive Officer\\nCEO Worldwide Amazon Stores\\nSenior Vice President and Chief Financial Officer\\nVice President, Worldwide Controller, and Principal Accounting Officer\\nCEO Amazon Web Services\\nSenior Vice President, General Counsel, and Secretary', 'output': 'David A. Zapolsky is the Senior Vice President, General Counsel and Secretary', 'page': '5'}\n"
     ]
    }
   ],
   "source": [
    "## 2. Download ground truth dataset\n",
    "\n",
    "import xmltodict\n",
    "url = 'https://d3q8adh3y5sxpk.cloudfront.net/rageval/qsdata_20.xml'\n",
    "\n",
    "# Send an HTTP GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:        \n",
    "    xml_data = xmltodict.parse(response.text)\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "qa_dataset = pd.DataFrame(xml_data['data']['records'])\n",
    "\n",
    "prompts = []\n",
    "for row in qa_dataset.itertuples():\n",
    "    item = {\n",
    "        'prompt': str(row[1]['Question']),\n",
    "        'context': str(row[1]['Context']),\n",
    "        'output': str(row[1]['Answer']['question_answer']),\n",
    "        'page': str(row[1]['Page'])\n",
    "    }\n",
    "    prompts.append(item)\n",
    "\n",
    "# example prompt\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae7ee402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTextSplitter split documents in to 354 chunks.\n",
      "\n",
      "CharacterTextSplitter split documents in to 828 chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Download context / Amazon annual report and create token_text_splitter and char_text_splitter for evaluation\n",
    "\n",
    "import numpy as np\n",
    "import pypdf\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [ \"https://d3q8adh3y5sxpk.cloudfront.net/rageval/AMZN-2023-10k.pdf\"]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)\n",
    "    \n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "\n",
    "token_text_list = token_text_splitter.split_documents(documents)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "    \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "print(\"CharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "196146f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host: lx0j8y3mu9ht6r5xv7za.us-east-1.aoss.amazonaws.com\n",
      "region: us-east-1\n",
      "aospy client:<OpenSearch([{'host': 'lx0j8y3mu9ht6r5xv7za.us-east-1.aoss.amazonaws.com', 'port': 443}])>\n"
     ]
    }
   ],
   "source": [
    "# 4. Create or get vector database (OpenSearch Serverless)\n",
    "\n",
    "import time\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "host = os.environ['OPENSEARCH_COLLECTION']  # serverless collection endpoint, without https://\n",
    "print(f\"host: {host}\")\n",
    "region = os.environ['REGION']  # e.g. us-east-1\n",
    "print(f'region: {region}')\n",
    "\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "## 4b. create vectordatabase if it does not exist yet\n",
    "if host == '':\n",
    "    print('creating collection')\n",
    "    vector_store_name = 'rag-eval'\n",
    "    encryption_policy_name = \"rag-eval-ep\"\n",
    "    network_policy_name = \"rag-eval-np\"\n",
    "    access_policy_name = 'rag-eval-ap'\n",
    "    identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "    aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "    security_policy = aoss_client.create_security_policy(\n",
    "        name = encryption_policy_name,\n",
    "        policy = json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type = 'encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name = network_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type = 'network'\n",
    "    )\n",
    "\n",
    "    collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "    while True:\n",
    "        status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "        if status in ('ACTIVE', 'FAILED'): \n",
    "            print(f'new collection {vector_store_name} created')\n",
    "            break\n",
    "        time.sleep(10)\n",
    "\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name = access_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type = 'data'\n",
    "    )\n",
    "\n",
    "    host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'\n",
    "    host = host.split(\":\")[0]\n",
    "    print(f'new aoss host: {host}')\n",
    "\n",
    "aospy_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")\n",
    "print(f'aospy client:{aospy_client}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5292f998-cb7d-408f-8942-f5bba5700a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Create and save prompt templates for evaluation\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "### Claude prompt templates\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "PROMPT_CLAUDE_1 = PromptTemplate(\n",
    "    template=prompt_template_claude_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_claude_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "\n",
    "        Assistant:\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "\n",
    "PROMPT_CLAUDE_2 = PromptTemplate(\n",
    "    template=prompt_template_claude_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "### Llama2 prompt templates\n",
    "prompt_template_llama_1 = \"\"\"\n",
    "        [INST] Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags. [/INST]\n",
    "        \"\"\"\n",
    "PROMPT_LLAMA_1 = PromptTemplate(\n",
    "    template=prompt_template_llama_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_llama_2 = \"\"\"\n",
    "        [INST]\n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "        [/INST]\n",
    "\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        [INST]\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "        [/INST]\n",
    "\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        [INST]\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "        [/INST]\n",
    "\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "PROMPT_LLAMA_2 = PromptTemplate(\n",
    "    template=prompt_template_llama_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "\n",
    "### Cohere Command prompt templates\n",
    "prompt_template_command_1 = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "PROMPT_COMMAND_1 = PromptTemplate(\n",
    "    template=prompt_template_command_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_command_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "\n",
    "        Assistant:\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "PROMPT_COMMAND_2 = PromptTemplate(\n",
    "    template=prompt_template_command_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# generic prompt template for all LLMs\n",
    "generic_rag_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompttemplates = [\n",
    "    {'template_name': 'generic_rag_template', 'template': generic_rag_template},\n",
    "    {'template_name': 'prompt_template_claude_1', 'template': PROMPT_CLAUDE_1},\n",
    "    {'template_name': 'prompt_template_claude_2', 'template': PROMPT_CLAUDE_2},\n",
    "    {'template_name': 'prompt_template_command_1', 'template': PROMPT_COMMAND_1},\n",
    "    {'template_name': 'prompt_template_command_2', 'template': PROMPT_COMMAND_2},\n",
    "    {'template_name': 'prompt_template_llama_1', 'template': PROMPT_LLAMA_1},\n",
    "    {'template_name': 'prompt_template_llama_2', 'template': PROMPT_LLAMA_2},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30244609",
   "metadata": {},
   "source": [
    "## 6) Create 2 helper functions:\n",
    "### 6a. create_rag to create RAG systems for evaluation\n",
    "### 6b. run_ragas_eval to evaluate RAG system with RAGAS and LlamaIndex evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88ca290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a. helper function to create RAG systems for evaluation\n",
    "\n",
    "import random\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings  \n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "# # LangChain requires AWS4Auth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "def get_aws4_auth():\n",
    "    region = os.environ.get(\"Region\", os.environ[\"REGION\"])\n",
    "    service = \"aoss\"\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    return AWS4Auth(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        region,\n",
    "        service,\n",
    "        session_token=credentials.token,\n",
    "    )\n",
    "aws4_auth = get_aws4_auth()\n",
    "\n",
    "\n",
    "def create_rag(rag_system_details):\n",
    "    existing_vector_store = rag_system_details[\"vector_store\"]\n",
    "    kb_id = rag_system_details[\"kb_id\"]\n",
    "    llm = rag_system_details[\"llm\"]\n",
    "    aospy_client = rag_system_details[\"aospy_client\"]\n",
    "    index_name = rag_system_details[\"index_name\"]\n",
    "    embedding_model = rag_system_details[\"embedding_model\"]\n",
    "    embedding_model_name = rag_system_details[\"embedding_model_name\"]\n",
    "    splitter_name = rag_system_details[\"splitter_name\"]\n",
    "    text_chunks = rag_system_details[\"text_chunks\"]\n",
    "    index_name = rag_system_details[\"index_name\"]\n",
    "    index_dimension = rag_system_details[\"index_dimension\"]\n",
    "    prompt_template_name = rag_system_details[\"prompt_template_name\"]\n",
    "    prompt_template = rag_system_details[\"prompt_template\"]\n",
    "    chain_type= rag_system_details[\"chain_type\"]\n",
    "    search_type= rag_system_details[\"search_type\"]\n",
    "    retriever_k = rag_system_details[\"retriever_k\"]\n",
    "    score_threshold = rag_system_details[\"score_threshold\"]\n",
    "    fetch_k = rag_system_details[\"fetch_k\"]\n",
    "    lambda_mult = rag_system_details[\"lambda_mult\"]\n",
    "    if existing_vector_store == \"\":\n",
    "        # create index\n",
    "        knn_index = {\n",
    "            \"settings\": {\n",
    "                \"index.knn\": True,\n",
    "                \n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"vector_field\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": index_dimension,\n",
    "                        \"store\": True\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"store\": True\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            aospy_client.indices.delete(index=index_name)\n",
    "            aospy_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "            aospy_client.indices.get(index=index_name)\n",
    "        except:\n",
    "            print(f'Index {index_name} not found. Creating index on OpenSearch.')\n",
    "            aospy_client.indices.create(index=index_name,body=knn_index)\n",
    "            aospy_client.indices.get(index=index_name)\n",
    "\n",
    "        # generate embeddings\n",
    "        full_opensearch_endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "        vector_store = OpenSearchVectorSearch.from_documents(\n",
    "                    index_name = index_name,\n",
    "                    documents = text_chunks,\n",
    "                    embedding = embedding_model,\n",
    "                    opensearch_url=full_opensearch_endpoint,\n",
    "                    http_auth=auth,\n",
    "                    use_ssl=True,\n",
    "                    verify_certs=True,\n",
    "                    connection_class=RequestsHttpConnection,\n",
    "                    timeout=60*3,\n",
    "                    bulk_size=1000,\n",
    "                    is_aoss=True\n",
    "                )  \n",
    "    else:\n",
    "        vector_store = existing_vector_store\n",
    "\n",
    "    random_identifier = random.randrange(100, 1000, 3)\n",
    "\n",
    "    if kb_id == \"\":\n",
    "        run_name=f'LLM_{llm.model_id}_embeddings{embedding_model_name}_split_{splitter_name}_template_{prompt_template_name}_search_{search_type}_chain_{chain_type}_k_{retriever_k}_{random_identifier}'\n",
    "        search_kwargs = {\n",
    "            \"retriever_k\": retriever_k\n",
    "        }\n",
    "        retriever = vector_store.as_retriever(search_type = search_type, search_kwargs=search_kwargs)\n",
    "    else:\n",
    "        # use Bedrock Knowledgebase retriever\n",
    "        run_name=f'LLM_{llm.model_id}_embeddings{embedding_model_name}_split_{splitter_name}_template_{prompt_template_name}_kb_id_{kb_id}_k_{retriever_k}_{random_identifier}'\n",
    "        retriever = AmazonKnowledgeBasesRetriever(\n",
    "                knowledge_base_id=kb_id,\n",
    "                retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": retriever_k}},\n",
    "\n",
    "            )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=chain_type,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs = {\"prompt\": prompt_template}\n",
    "    )\n",
    "    \n",
    "    return run_name, vector_store, qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73a13c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. helper function to evaluate RAG system with RAGAS and LlamaIndexEval\n",
    "\n",
    "import mlflow\n",
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "from datasets import Dataset\n",
    "import ragas\n",
    "#import tqdm as notebook_tqdm\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "\n",
    "# llama_index evaluators\n",
    "## Faithfulness: measure if the response from a query engine matches any source nodes\n",
    "## Relevancy: measure if the response and source nodes match the query\n",
    "## Correctness: assess the relevance and correctness of a generated answer against a reference answer\n",
    "## Semantic Similarity: evaluates the quality of a question answering system via semantic similarity\n",
    "\n",
    "from llama_index.llms import Bedrock\n",
    "from llama_index.embeddings import BedrockEmbedding\n",
    "from llama_index import (\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "from llama_index.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.embeddings import SimilarityMode\n",
    "from llama_index import Document\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "#LLM_EVAL_NAME= \"meta.llama2-70b-chat-v1\"\n",
    "llamaindex_eval_llm = Bedrock(model=\"anthropic.claude-v2\",\n",
    "              #context_size=512,\n",
    "              temperature=0,\n",
    "              additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "llamaindex_embed_model = BedrockEmbedding().from_credentials(\n",
    "    model_name='amazon.titan-embed-g1-text-02'\n",
    ")\n",
    "\n",
    "service_context_eval = ServiceContext.from_defaults(\n",
    "    llm=llamaindex_eval_llm, \n",
    "    embed_model=llamaindex_embed_model, \n",
    ")\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_eval)\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_eval)\n",
    "similarity_threshold = 0.8\n",
    "semantic_evaluator = SemanticSimilarityEvaluator(service_context=service_context_eval,\n",
    "                                                 similarity_mode=SimilarityMode.DEFAULT,\n",
    "                                                 similarity_threshold=similarity_threshold) # 0.8 default\n",
    "correctness_evaluator = CorrectnessEvaluator(service_context=service_context_eval)\n",
    "\n",
    "\n",
    "def run_ragas_eval(rag_system_eval_details, rag_system_details):\n",
    "\n",
    "    experiment_name = rag_system_eval_details[\"experiment_name\"]\n",
    "    run_name = rag_system_eval_details[\"run_name\"]\n",
    "    qa_chain = rag_system_eval_details[\"qa_chain\"]\n",
    "    ground_truth = rag_system_eval_details[\"ground_truth\"]\n",
    "\n",
    "    llm_experiment = mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Initiate the MLflow run context\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        # list of metrics we're going to use from RAGAS\n",
    "        metrics = [\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            context_recall,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness\n",
    "            \n",
    "        ]\n",
    "\n",
    "        basic_qa_ragas_dataset = []\n",
    "        basic_qa_llamaindex_dataset = []\n",
    "\n",
    "        for item in ground_truth:\n",
    "            result = qa_chain.invoke(item['prompt'])\n",
    "\n",
    "            context_sequence = []\n",
    "            for doc in result[\"source_documents\"]:\n",
    "                context_sequence.append(doc.page_content)\n",
    "\n",
    "            basic_qa_ragas_dataset.append(\n",
    "                    {\"question\" : item['prompt'],\n",
    "                    \"answer\" : result[\"result\"],\n",
    "                    \"contexts\" : context_sequence,\n",
    "                    \"ground_truths\" : [item['output']]\n",
    "                    }\n",
    "                )\n",
    "            basic_qa_ragas_df = pd.DataFrame(basic_qa_ragas_dataset)\n",
    "            basic_qa_ragas = Dataset.from_pandas(basic_qa_ragas_df)\n",
    "\n",
    "            # evaluate with LlamaIndexEvaluators\n",
    "            question = item['prompt']\n",
    "            reference_answer = item['output']\n",
    "            generated_answer = result[\"result\"]\n",
    "            retrieved_context = context_sequence\n",
    "\n",
    "\n",
    "            faithfulness_results = faithfulness_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                contexts=retrieved_context\n",
    "                )\n",
    "            \n",
    "            relevancy_results = relevancy_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                contexts=retrieved_context\n",
    "                )\n",
    "            \n",
    "                \n",
    "            semantic_results = semantic_evaluator.evaluate(\n",
    "                response=generated_answer,\n",
    "                reference=reference_answer\n",
    "            )\n",
    "\n",
    "            correctness_results = correctness_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                reference=reference_answer\n",
    "            )\n",
    "\n",
    "            basic_qa_llamaindex_dataset.append(\n",
    "                { \n",
    "                    \"correctness_score\" : correctness_results.score,\n",
    "                    \"similarity_score\" : semantic_results.score,\n",
    "                    \"faithfulness_score\" : faithfulness_results.score,\n",
    "                    \"relevancy_score\" : relevancy_results.score\n",
    "                }\n",
    "\n",
    "            )\n",
    "        \n",
    "        \n",
    "        basic_qa_ragas_df.to_csv('./mlflow_run_predictions.csv',index=False)\n",
    "        mlflow_dataset = mlflow.data.from_pandas(basic_qa_ragas_df, source='./mlflow_run_predictions.csv')\n",
    "\n",
    "        # Log input dataset to MLflow Tracking as a JSON artifact.\n",
    "        mlflow.log_input(mlflow_dataset, context=\"rag-output\")\n",
    "\n",
    "        # evaluate with RAGAS\n",
    "        ragas_result = evaluate(basic_qa_ragas, metrics=metrics)\n",
    "        ragas_evals_df = ragas_result.to_pandas()\n",
    "\n",
    "        # Log RAGAS results to MLflow Tracking as a JSON artifact.\n",
    "        mlflow.log_table(data=ragas_evals_df, artifact_file=\"qa_ragas_eval_results.json\")\n",
    "\n",
    "        # convert LlamaEvaluation results to df\n",
    "        basic_qa_llamaindex_dataset_df = pd.DataFrame(basic_qa_llamaindex_dataset)\n",
    "\n",
    "        # Log LlamaEvaluation results to MLflow Tracking as a JSON artifact.\n",
    "        mlflow.log_table(data=basic_qa_llamaindex_dataset_df, artifact_file=\"qa_llamaindex_eval_results.json\")\n",
    "\n",
    "        # Log parameters used for the RAG system\n",
    "        params = {\n",
    "            \"llm_name\": rag_system_details[\"llm\"].model_id,\n",
    "            \"kb_id\": rag_system_details[\"kb_id\"],\n",
    "            \"splitter_name\": rag_system_details[\"splitter_name\"],\n",
    "            \"index_name\": rag_system_details[\"index_name\"],\n",
    "            \"index_dimension\": rag_system_details[\"index_dimension\"],\n",
    "            \"embedding_model_name\": rag_system_details[\"embedding_model_name\"],\n",
    "            \"prompt_template_name\": rag_system_details[\"prompt_template_name\"],\n",
    "            \"chain_type\": rag_system_details[\"chain_type\"],\n",
    "            \"search_type\": rag_system_details[\"search_type\"],\n",
    "            \"retriever_k\": rag_system_details[\"retriever_k\"],\n",
    "            \"score_threshold\": rag_system_details[\"score_threshold\"],\n",
    "            \"fetch_k\": rag_system_details[\"fetch_k\"],\n",
    "            \"lambda_mult\": rag_system_details[\"lambda_mult\"]\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        print(f'ragas_faithfulness mean: {ragas_evals_df[\"faithfulness\"].mean()}')\n",
    "        print(f'ragas_answer_relevancy mean: {ragas_evals_df[\"answer_relevancy\"].mean()}')\n",
    "        print(f'ragas_answer_similarity mean: {ragas_evals_df[\"answer_similarity\"].mean()}')\n",
    "        print(f'ragas_answer_correctness mean: {ragas_evals_df[\"answer_correctness\"].mean()}')\n",
    "        print(f'ragas_context_recall: {ragas_evals_df[\"context_recall\"].mean()}')\n",
    "        print(f'ragas_context_precision: {ragas_evals_df[\"context_precision\"].mean()}')\n",
    "\n",
    "        mlflow_metrics_results = {\n",
    "            \"ragas_faithfulness_mean\": ragas_evals_df[\"faithfulness\"].mean(),\n",
    "            \"ragas_answer_relevancy_mean\": ragas_evals_df[\"answer_relevancy\"].mean(),\n",
    "            \"ragas_answer_similarity_mean\": ragas_evals_df[\"answer_similarity\"].mean(),\n",
    "            \"ragas_answer_correctness_mean\": ragas_evals_df[\"answer_correctness\"].mean(),\n",
    "            \"ragas_context_recall_mean\": ragas_evals_df[\"context_recall\"].mean(),\n",
    "            \"ragas_context_precision_mean\": ragas_evals_df[\"context_precision\"].mean(),\n",
    "\n",
    "            \"llamaindex_correctness_score_mean\": basic_qa_llamaindex_dataset_df[\"correctness_score\"].mean(),\n",
    "            \"llamaindex_semantic_similarity_score_mean\": basic_qa_llamaindex_dataset_df[\"similarity_score\"].mean(),\n",
    "            \"llamaindex_faithfulness_score_mean\": basic_qa_llamaindex_dataset_df[\"faithfulness_score\"].mean(),\n",
    "            \"llamaindex_relevancy_score_mean\": basic_qa_llamaindex_dataset_df[\"relevancy_score\"].mean()\n",
    "\n",
    "        }\n",
    "        # Log evaluation metrics that were calculated\n",
    "        mlflow.log_metrics(mlflow_metrics_results)\n",
    "\n",
    "        return mlflow_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "da7d2b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index rag-eval-tokentextsplitter not found. Creating index on OpenSearch.\n",
      "{'query': \"Who is Amazon's Senior Vice President and General Counsel?\", 'result': ' <question_answer>\\nDavid Zapolsky\\n</question_answer>', 'source_documents': []}\n"
     ]
    }
   ],
   "source": [
    "# 7. test a RAG system\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "query = \"Who is Amazon's Senior Vice President and General Counsel?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad781",
   "metadata": {},
   "source": [
    "## 8) RAG system component evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3cd04c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'index_dimension'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 1. Text Splitter: TokenTextSplitter vs CharacterTextSplitter Eval\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m## 1a. create RAG system with TokenTextSplitter and then run RAG system evaluation and log it in mlfow\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m rag_system_details \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39maospy_client\u001b[39m\u001b[39m\"\u001b[39m: aospy_client,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvector_store\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m          \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m run_name, vector_store, qa_chain \u001b[39m=\u001b[39m create_rag(rag_system_details)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m rag_system_eval_details \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m\"\u001b[39m: experiment_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m: run_name,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mqa_chain\u001b[39m\u001b[39m\"\u001b[39m: qa_chain,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mground_truth\u001b[39m\u001b[39m\"\u001b[39m: prompts\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m ragas_eval_result \u001b[39m=\u001b[39m run_ragas_eval(rag_system_eval_details, rag_system_details)\n",
      "\u001b[1;32m/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m text_chunks \u001b[39m=\u001b[39m rag_system_details[\u001b[39m\"\u001b[39m\u001b[39mtext_chunks\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m index_name \u001b[39m=\u001b[39m rag_system_details[\u001b[39m\"\u001b[39m\u001b[39mindex_name\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m index_dimension \u001b[39m=\u001b[39m rag_system_details[\u001b[39m\"\u001b[39;49m\u001b[39mindex_dimension\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m prompt_template_name \u001b[39m=\u001b[39m rag_system_details[\u001b[39m\"\u001b[39m\u001b[39mprompt_template_name\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/huthmac/Documents/AWS/00_workspace/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m prompt_template \u001b[39m=\u001b[39m rag_system_details[\u001b[39m\"\u001b[39m\u001b[39mprompt_template\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'index_dimension'"
     ]
    }
   ],
   "source": [
    "# 1. Text Splitter: TokenTextSplitter vs CharacterTextSplitter Eval\n",
    "## 1a. create RAG system with TokenTextSplitter and then run RAG system evaluation and log it in mlfow\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1b. create RAG system with CharTextSplitter and then run RAG system evaluation and log it in mlfow\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763e48a",
   "metadata": {},
   "source": [
    "## 1c. Review RecursiveCharacterTextSplitter vs TokenTextSplitter results\n",
    "Metrics such as context_recall are still low, but improved with the character text splitter.\n",
    "\n",
    "<img src=\"./images/splitter_results.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e789e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Embedding model: amazon.titan-embed-text-v1 vs cohere.embed-multilingual-v3\n",
    "## 2a. Create RAG system with amazon.titan-embed-text-v1\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter_1536\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'amazon.titan-embed-text-v1',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2b. Create RAG system with cohere.embed-multilingual-v3\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85354e91",
   "metadata": {},
   "source": [
    "## 2c. Review Embedding model: amazon.titan-embed-text-v1 vs cohere.embed-multilingual-v3 results\n",
    "context_recall is still low, but improved with the cohere embedding model.\n",
    "\n",
    "<img src=\"./images/embedding_results.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. retriever: OpenSearch VectoreStoreRetriever search types “similarity” vs “mmr”\n",
    "## 3a. Create RAG system with “similarity”\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3b. Create RAG system with mmr\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55649b25",
   "metadata": {},
   "source": [
    "## 3c. Review “similarity” vs “mmr” results\n",
    "Metrics such as faithfullness are still low, but improved with mmr.\n",
    "\n",
    "<img src=\"./images/searchtype_results.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4366349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prompt Template: For each LLM we evaluate two different prompt templates\n",
    "## 4a. Create RAG system with PROMPT_CLAUDE_1\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c597f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4b. Create RAG system with PROMPT_CLAUDE_2\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_2\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_2,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_result = run_ragas_eval(rag_system_eval_details, rag_system_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3e507",
   "metadata": {},
   "source": [
    "## 4c. Review PROMPT_CLAUDE_1 vs PROMPT_CLAUDE_2 results\n",
    "Prompt Template PROMPT_CLAUDE_2 performed better than PROMPT_CLAUDE_1.\n",
    "\n",
    "<img src=\"./images/prompttemplate_results.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814cc8f6",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea56a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA_INDEX EVAL\n",
    "\n",
    "## use results from LLMInformationExtraction.ipynb\n",
    "### query,llm,output,trainingoutput,context,trainingcontext,evaluationmetric,score,feedback\n",
    "predictions_df = pd.read_csv('eval_run_predictions.csv')\n",
    "print(f'column names: {predictions_df.columns}')\n",
    "print(f'no of rows: {predictions_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d363ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation directly with llama_index on an existing dataframe\n",
    "## Faithfulness: measure if the response from a query engine matches any source nodes\n",
    "## Relevancy: measure if the response and source nodes match the query\n",
    "## Correctness: assess the relevance and correctness of a generated answer against a reference answer\n",
    "## Semantic Similarity: evaluates the quality of a question answering system via semantic similarity\n",
    "\n",
    "from llama_index.llms import Bedrock\n",
    "from llama_index.embeddings import BedrockEmbedding\n",
    "from llama_index import (\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "from llama_index.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.embeddings import SimilarityMode\n",
    "from llama_index import Document\n",
    "\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "#LLM_EVAL_NAME= \"meta.llama2-70b-chat-v1\"\n",
    "eval_llm = Bedrock(model=\"anthropic.claude-v2\",\n",
    "              #context_size=512,\n",
    "              temperature=0,\n",
    "              additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "embed_model = BedrockEmbedding().from_credentials(\n",
    "    model_name='amazon.titan-embed-g1-text-02'\n",
    ")\n",
    "\n",
    "service_context_eval = ServiceContext.from_defaults(\n",
    "    llm=eval_llm, \n",
    "    embed_model=embed_model, \n",
    ")\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_eval)\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_eval)\n",
    "similarity_threshold = 0.8\n",
    "semantic_evaluator = SemanticSimilarityEvaluator(service_context=service_context_eval,\n",
    "                                                 similarity_mode=SimilarityMode.DEFAULT,\n",
    "                                                 similarity_threshold=similarity_threshold) # 0.8 default\n",
    "correctness_evaluator = CorrectnessEvaluator(service_context=service_context_eval) # encountered parsing errors with this class\n",
    "\n",
    "def run_evals(qa_df):\n",
    "    results_list = []\n",
    "    for row in qa_df.itertuples(index=False):\n",
    "        question = row.query\n",
    "        reference_answer = row.trainingoutput\n",
    "        generated_answer = row.output\n",
    "        retrieved_context = row.context.replace('[]','')\n",
    "        retrieved_context = retrieved_context.split(\"/n\")\n",
    "        #print(f'retrieved context: {retrieved_context}')\n",
    "        #print(f'retrieved context type: {type(retrieved_context)}')\n",
    "\n",
    "        faithfulness = False\n",
    "        faithfulness_feedback  = 'not calculated'\n",
    "        faithfulness_score =  0.0\n",
    "        relevancy = False\n",
    "        relevancy_feedback =  'not calculated'\n",
    "        relevancy_score  =  0.0\n",
    "        correctness = False\n",
    "        correctness_feedback = 'not calculated'\n",
    "        correctness_score = 1.0\n",
    "        \n",
    "        if not(len(retrieved_context) == 0 or retrieved_context[0] == ''):\n",
    "\n",
    "            faithfulness_results = faithfulness_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                contexts=retrieved_context\n",
    "                )\n",
    "            \n",
    "            relevancy_results = relevancy_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                contexts=retrieved_context\n",
    "                )\n",
    "            faithfulness = faithfulness_results.passing\n",
    "            faithfulness_feedback  = faithfulness_results.feedback\n",
    "            faithfulness_score =  faithfulness_results.score\n",
    "            relevancy = relevancy_results.passing\n",
    "            relevancy_feedback =  relevancy_results.feedback\n",
    "            relevancy_score  =  relevancy_results.score\n",
    "            \n",
    "        semantic_results = semantic_evaluator.evaluate(\n",
    "            response=generated_answer,\n",
    "            reference=reference_answer\n",
    "        )\n",
    "\n",
    "        # correctness_results = correctness_evaluator.evaluate(\n",
    "        #     query=question,\n",
    "        #     response=generated_answer,\n",
    "        #     reference=reference_answer\n",
    "        # )\n",
    "\n",
    "        # correctness= correctness_results.passing\n",
    "        # correctness_feedback= correctness_results.feedback\n",
    "        # correctness_score= correctness_results.score\n",
    "\n",
    "        cur_result_dict = {\n",
    "            \"query\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"correctness\": correctness,\n",
    "            \"correctness_feedback\": correctness_feedback,\n",
    "            \"correctness_score\": correctness_score,\n",
    "            \"semantic_similarity\": semantic_results.passing,\n",
    "            \"semantic_similarity_threshold\": similarity_threshold,\n",
    "            \"semantic_similarity_score\": semantic_results.score,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"faithfulness_feedback\": faithfulness_feedback,\n",
    "            \"faithfulness_score\": faithfulness_score,\n",
    "            \"relevancy\": relevancy,\n",
    "            \"relevancy_feedback\": relevancy_feedback,\n",
    "            \"relevancy_score\": relevancy_score\n",
    "        }\n",
    "        results_list.append(cur_result_dict)\n",
    "    evals_df = pd.DataFrame(results_list)\n",
    "    return evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST LLAMA_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ee3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://d3q8adh3y5sxpk.cloudfront.net/rageval/AMZN-2023-10k.pdf',\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2023-10k.pdf',\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    get_response_synthesizer,\n",
    "    set_global_service_context\n",
    ")\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b426d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Bedrock\n",
    "from llama_index.embeddings import BedrockEmbedding\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "llm = Bedrock(model=\"anthropic.claude-v2\",\n",
    "              #context_size=512,\n",
    "              temperature=0,\n",
    "              additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "embed_model = BedrockEmbedding().from_credentials(\n",
    "    model_name='amazon.titan-embed-g1-text-02'\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embed_model, \n",
    "                                               chunk_size=512)\n",
    "chunk_overlap = 20\n",
    "chunk_size = 512\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embed_model, \n",
    "                                               chunk_size=chunk_size,\n",
    "                                               chunk_overlap=chunk_overlap,\n",
    "                                            )\n",
    "set_global_service_context(service_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_fn = lambda filename: {\"file_path\": filename, \"file_name\": filename.replace('data/', \"\").replace('.pdf', \"\")}\n",
    "\n",
    "# automatically sets the metadata of each document according to filename_fn\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", file_metadata=filename_fn\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b924b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review metadata\n",
    "print(documents[50].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.vector_stores import (\n",
    "    OpensearchVectorStore,\n",
    "    OpensearchVectorClient,\n",
    ")\n",
    "from llama_index import VectorStoreIndex, StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc787db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "host = os.environ['OPENSEARCH_COLLECTION'] # OpenSearch endpoint, for example: my-test-domain.us-east-1.aoss.amazonaws.com\n",
    "service = 'aoss'\n",
    "region = 'us-east-1'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "print(f'endpoint: {endpoint}')\n",
    "index_name = \"rag-eval-v1\"\n",
    "# OpensearchVectorClient stores text in this field by default\n",
    "text_field = \"content\"\n",
    "# OpensearchVectorClient stores embeddings in this field by default\n",
    "embedding_field = \"embedding\"\n",
    "\n",
    "client = OpensearchVectorClient(\n",
    "    endpoint=endpoint,\n",
    "    index=index_name, \n",
    "    dim=1536, \n",
    "    embedding_field=embedding_field, \n",
    "    text_field=text_field,\n",
    "    http_auth=auth, \n",
    "    use_ssl=True, \n",
    "    verify_certs=True, \n",
    "    connection_class=RequestsHttpConnection, \n",
    "    timeout=10,\n",
    ")\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vector store\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# initialize an index using our sample data and the client we just created\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ce1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run query\n",
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"Who is Amazon's Senior Vice President and General Counsel?\")\n",
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query with filtering - NOT WORKING ATM\n",
    "from llama_index import Document\n",
    "from llama_index.vector_stores.types import MetadataFilters, ExactMatchFilter, MetadataFilter,FilterOperator\n",
    "import regex as re\n",
    "\n",
    "# Create a query engine that only searches certain documents.\n",
    "metadata_query_engine = index.as_query_engine(\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            ExactMatchFilter(\n",
    "                key=\"term\", value='{\"file_path\": \"data/AMZN-2023-10k.pdf\"}'\n",
    "            )\n",
    "            #ExactMatchFilter(key=\"file_name\", value=\"AMZN-2023-10k\")\n",
    "            \n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "res = metadata_query_engine.query(\n",
    "    \"who is Amazon's Senior Vice President and General Counsel?\"\n",
    ")\n",
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c60cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Bedrock Knowledgebase retriever\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "kb_id = \"<knowledge_base_id>\"\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb_id,\n",
    "        retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    "\n",
    "    )\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": claude_prompt}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "rag-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
