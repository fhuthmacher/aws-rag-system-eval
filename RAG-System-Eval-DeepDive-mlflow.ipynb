{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8da33c-eeb6-4d72-a0fc-89f9fb6a19fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RAG System Component Evaluation\n",
    "    \n",
    "This notebook is a follow up from the previous [notebook](https://github.com/fhuthmacher/LLMevaluation/blob/main/LLMInformationExtraction.ipynb) and [youtube video](https://www.youtube.com/watch?v=HUuO9eJbOTk) in which we explored the overall evaluation approach and a RAG system's overall accuracy.\n",
    "\n",
    "In this notebook we will take a closer look at specific RAG evaluation metrics and explore how to optimize different RAG components based these RAG evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47020c-9266-4740-99c2-0d330b90d460",
   "metadata": {},
   "source": [
    "## Solution architecture\n",
    "<img src=\"./images/architecture.png\" alt=\"RAG eval architecture \" width=\"800\">\n",
    "\n",
    "From the solution architecture, we will experiment with the below RAG components and parameters to optimize our RAG evalution metrics.\n",
    "\n",
    "- Chunk Size: determines how a source text is stored in a knowledge base\n",
    "- Chunking strategy / Text Splitter: TokenTextSplitter vs CharacterTextSplitter\n",
    "- Embedding model: amazon.titan-embed-text-v1 vs cohere.embed-multilingual-v3 \n",
    "- Retriever: OpenSearch VectoreStoreRetriever search types “similarity” vs “mmr” vs Bedrock KnowledgeBase retriever\n",
    "- Prompt Template: set of instructions to guide the LLM’s response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4514e6",
   "metadata": {},
   "source": [
    "## RAG evaluation metrics\n",
    "\n",
    "This notebook covers the following metrics from Retrieval Augmented Generation Assessment (RAGAS):\n",
    "\n",
    "-  context_precision: evaluates whether all of the ground-truth relevant items are present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. Values range from 0 to 1, higher scores indicate better precision.\n",
    "-  context_recall: measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. Values range from 0 to 1, with higher values indicating better performance.\n",
    "-  answer_relevancy: focuses on assessing how relevant the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information. This metric is computed using the question and the answer, with values ranging between 0 and 1 where higher scores indicate better relevancy.\n",
    "-  answer_correctness¶: the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1  where higher scores indicate better relevancy.\n",
    "-  faithfulness: measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. Values range from 0 to 1, with higher values indicating better performance.\n",
    "\n",
    "Further information on RAG evaluation metrics with RAGAS can be found [here](https://docs.ragas.io/en/latest/concepts/metrics/index.html).\n",
    "\n",
    "\n",
    "While this notebook tracks metrics with mlflow, it is not required for the evaluation. If you want to learn more about how to configure mlflow in your environment, please refer to this [AWS Blog](https://aws.amazon.com/blogs/machine-learning/securing-mlflow-in-aws-fine-grained-access-control-with-aws-native-services/).\n",
    "\n",
    "In this notebook we use helper functions for illustration purposes. In production, this functionality would typically be part of an evaluation pipeline for example with Amazon SageMaker Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b3b3a",
   "metadata": {},
   "source": [
    "<img src=\"./images/RAGevalmetrics.png\" alt=\"RAG eval metrics\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5de0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The environment 'rag_eval' is present.\n"
     ]
    }
   ],
   "source": [
    "# parse output of !conda env list and check if value rag_eval is part of it\n",
    "import subprocess\n",
    "\n",
    "# Run the command and capture its output\n",
    "output = subprocess.check_output([\"conda\", \"env\", \"list\"])\n",
    "\n",
    "# Decode the output from bytes to string\n",
    "output = output.decode(\"utf-8\")\n",
    "\n",
    "# Split the output into lines\n",
    "lines = output.split(\"\\n\")\n",
    "\n",
    "# Iterate through the lines to find the environment names\n",
    "environment_names = []\n",
    "for line in lines:\n",
    "    # Skip empty lines and lines not starting with a hash (#)\n",
    "    if line.startswith(\"#\") or not line.strip():\n",
    "        continue\n",
    "    # Split the line by whitespace and get the first element (environment name)\n",
    "    environment_name = line.split()[0]\n",
    "    environment_names.append(environment_name)\n",
    "\n",
    "# Check if \"rag_eval\" is in the list of environment names\n",
    "if \"rag_eval\" in environment_names:\n",
    "    print(\"The environment 'rag_eval' is present.\")\n",
    "else:\n",
    "    print(\"The environment 'rag_eval' is not present.\")\n",
    "    # Create a conda environment to encapsulate dependencies\n",
    "    !conda env list\n",
    "    !conda create -y --name rag_eval python=3.11.8 \n",
    "    !conda init && activate rag_eval\n",
    "    !conda install -y ipykernel # if this fails, try running steps in terminal conda install -n rag_eval ipykernel --update-deps --force-reinstall\n",
    "\n",
    "    # After installing new conda env, do a browser refresh and select new kernel and restart JupyterLab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d744babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages are installed\n"
     ]
    }
   ],
   "source": [
    "# check installed packages\n",
    "with open('requirements.txt') as f:\n",
    "  requirements = f.readlines()\n",
    "\n",
    "import pkg_resources\n",
    "\n",
    "installed_packages = [d for d in pkg_resources.working_set]\n",
    "\n",
    "installed_packages = [package.project_name for package in installed_packages]\n",
    "\n",
    "missing_packages = []\n",
    "\n",
    "for requirement in requirements:\n",
    "  requirement = requirement.strip().split('==')[0]\n",
    "\n",
    "if missing_packages:\n",
    "  print(f\"Missing packages: {', '.join(missing_packages)}\")\n",
    "else: \n",
    "  print(\"All packages are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea6dd86c-115c-49b9-b112-548edf312d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if there are any missing dependencies, install packages\n",
    "if missing_packages:\n",
    "    # install dependencies\n",
    "    %pip install --force-reinstall --no-cache -r requirements.txt\n",
    "    !pip install --upgrade jupyter-console\n",
    "    !pip install --upgrade jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba52a1c8-3b25-4bfa-bbff-5bbbd5de31c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel to ensure proper version of libraries is loaded\n",
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "if missing_packages:\n",
    "    restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b6646c5-8ccf-4e0b-8ea8-0501c379d551",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3                     1.34.87\n",
      "botocore                  1.34.93\n",
      "langchain                 0.1.16\n",
      "langchain-aws             0.1.2\n",
      "langchain-community       0.0.34\n",
      "langchain-core            0.1.50\n",
      "langchain-openai          0.1.6\n",
      "langchain-text-splitters  0.0.1\n",
      "langchainhub              0.1.15\n",
      "nest-asyncio              1.6.0\n",
      "nltk                      3.8.1\n",
      "opensearch-py             2.4.2\n",
      "plotly                    5.9.0\n",
      "pypdf                     3.17.4\n",
      "python-dotenv             1.0.0\n",
      "ragas                     0.1.7\n",
      "requests-aws4auth         1.2.3\n",
      "sagemaker                 2.203.1\n",
      "tiktoken                  0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -E \"awscli|boto3|botocore|langchain|llama-index|plotly|tiktoken|nltk|python-dotenv|ragas|requests-aws4auth|pypdf|opensearch-py|sagemaker|nest-asyncio\"\n",
    "# also review requirements.txt for specific package versions if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e938c8c-5e0b-483c-9efd-84c7c9d4e3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load environment variables \n",
    "import boto3\n",
    "import os\n",
    "import botocore\n",
    "from botocore.config import Config\n",
    "import langchain\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from typing import Dict\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "import random\n",
    "random_identifier = random.randint(100,999)\n",
    "\n",
    "# loading environment variables that are stored in local file dev.env\n",
    "load_dotenv(find_dotenv('dev-rageval.env'),override=True)\n",
    "\n",
    "os.environ['OPENSEARCH_COLLECTION'] = os.getenv('OPENSEARCH_COLLECTION')\n",
    "os.environ['BEDROCK_KNOWLEDGEBASE_ID'] = os.getenv('BEDROCK_KNOWLEDGEBASE_ID')\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['MLFLOW_TRACKING_ENABLED'] = os.getenv('MLFLOW_TRACKING_ENABLED')\n",
    "os.environ['MLFLOW_TRACKING_URI'] = os.getenv('MLFLOW_TRACKING_URI')\n",
    "\n",
    "if os.environ['MLFLOW_TRACKING_ENABLED'] == 'True':\n",
    "   # Initialize mlflow client\n",
    "   mlflow_client = MlflowClient(tracking_uri=os.environ['MLFLOW_TRACKING_URI'])\n",
    "\n",
    "\n",
    "# Initialize Bedrock runtime\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 10,\n",
    "      'mode': 'standard'\n",
    "   }\n",
    ")\n",
    "bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        config=config\n",
    ")\n",
    "\n",
    "# Initialize sagemaker session\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "977002d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k', 'modelId': 'anthropic.claude-instant-v1:2:100k', 'modelName': 'Claude Instant', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1', 'modelId': 'anthropic.claude-instant-v1', 'modelName': 'Claude Instant', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:18k', 'modelId': 'anthropic.claude-v2:0:18k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:100k', 'modelId': 'anthropic.claude-v2:0:100k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:18k', 'modelId': 'anthropic.claude-v2:1:18k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:200k', 'modelId': 'anthropic.claude-v2:1:200k', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1', 'modelId': 'anthropic.claude-v2:1', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2', 'modelId': 'anthropic.claude-v2', 'modelName': 'Claude', 'providerName': 'Anthropic', 'inputModalities': ['TEXT'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k', 'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k', 'modelName': 'Claude 3 Sonnet', 'providerName': 'Anthropic', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k', 'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k', 'modelName': 'Claude 3 Sonnet', 'providerName': 'Anthropic', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0', 'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0', 'modelName': 'Claude 3 Sonnet', 'providerName': 'Anthropic', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k', 'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k', 'modelName': 'Claude 3 Haiku', 'providerName': 'Anthropic', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k', 'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k', 'modelName': 'Claude 3 Haiku', 'providerName': 'Anthropic', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['PROVISIONED'], 'modelLifecycle': {'status': 'ACTIVE'}}\n",
      "{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0', 'modelId': 'anthropic.claude-3-haiku-20240307-v1:0', 'modelName': 'Claude 3 Haiku', 'providerName': 'Anthropic', 'inputModalities': ['TEXT', 'IMAGE'], 'outputModalities': ['TEXT'], 'responseStreamingSupported': True, 'customizationsSupported': [], 'inferenceTypesSupported': ['ON_DEMAND'], 'modelLifecycle': {'status': 'ACTIVE'}}\n"
     ]
    }
   ],
   "source": [
    "# call bedrock to return list of all available models\n",
    "bedrock_client = boto3.client(\n",
    "        service_name=\"bedrock\",\n",
    "        config=config\n",
    ")\n",
    "response = bedrock_client.list_foundation_models()\n",
    "for model in response['modelSummaries']:\n",
    "        if model['modelLifecycle']['status'] == 'ACTIVE' and model['providerName'] == 'Anthropic': \n",
    "                print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd9a12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mflow is disabled\n"
     ]
    }
   ],
   "source": [
    "# Create a new mlflow experiment\n",
    "import random\n",
    "\n",
    "\n",
    "if os.environ['MLFLOW_TRACKING_ENABLED'] == 'True':\n",
    "    experiment_description = (\n",
    "        \"RAG system evaluation project.\"\n",
    "    )\n",
    "\n",
    "    experiment_tags = {\n",
    "        \"project_name\": \"rag-eval\",\n",
    "        \"use_case\": \"information extraction\",\n",
    "        \"team\": \"aws-ai-ml-analytics\",\n",
    "        \"source\": \"Amazon 10k\",\n",
    "        \"mlflow.note.content\": experiment_description,\n",
    "    }\n",
    "\n",
    "    experiment_name = f\"RAG_system_accuracy_{random_identifier}\"\n",
    "\n",
    "    # Use search_experiments() to search on the project_name tag key\n",
    "    rag_experiment = mlflow_client.search_experiments(\n",
    "        filter_string=\"tags.`project_name` = 'rag-eval'\"\n",
    "    )\n",
    "    # check length of mlflow.store.entities.paged_list.PagedList\n",
    "    # if experiment does not exist yet, create it\n",
    "    if len(rag_experiment) == 0:\n",
    "        rag_experiment = mlflow_client.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "        print(rag_experiment)\n",
    "else:\n",
    "    print(f'mflow is disabled')\n",
    "    experiment_name = 'not_tracked'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9f1edb6-2102-48a2-bede-ffabd3caa462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Initialize LLMs (Claude, Cohere, LLama)\n",
    "\n",
    "## 1a. Initialize Claude\n",
    "llm01_inference_modifier = {\n",
    "    \"max_tokens\": 545, # Claude-3 use “max_tokens” However Claude-2 requires “max_tokens_to_sample”\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 20,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "from langchain_aws import BedrockLLM #ChatBedrock #BedrockLLM\n",
    "LLM_01_NAME = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "llm01 = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=LLM_01_NAME,\n",
    "    model_kwargs=llm01_inference_modifier\n",
    ")\n",
    "\n",
    "## 1b. Initialize Cohere Command\n",
    "llm02_inference_modifier = { \n",
    "    \"max_tokens\": 545,\n",
    "    \"top_k\": 20,\n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_02_NAME= \"cohere.command-text-v14\"\n",
    "llm02 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_02_NAME,\n",
    "    model_kwargs=llm02_inference_modifier\n",
    ")\n",
    "\n",
    "## 1c. Initialize Llama\n",
    "llm03_inference_modifier = { \n",
    "    \"max_gen_len\": 545,\n",
    "    \"top_p\": 0.9, \n",
    "    \"top_k\": 20,\n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_03_NAME= \"meta.llama2-13b-chat-v1\"\n",
    "llm03 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_03_NAME,\n",
    "    model_kwargs=llm03_inference_modifier\n",
    ")\n",
    "\n",
    "llms = [\n",
    "    llm01,\n",
    "    llm02,\n",
    "    llm03\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60611855-58f1-4568-b250-88ed113262e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"Who is Amazon's Senior Vice President and General Counsel?\", 'context': 'Available Information\\nOur investor relations website is amazon.com/ir and we encourage investors to use it as a way of easily finding information about us. We promptly make available on this website, free of charge, the reports that we file or furnish with the Securities and Exchange Commission (â\\x80\\x9cSECâ\\x80\\x9d), corporate governance information (including our Code of Business Conduct and Ethics), and select press releases.\\nExecutive Officers and Directors\\nThe following tables set forth certain information regarding our Executive Officers and Directors as of January 25, 2023:\\nInformation About Our Executive Officers\\nName Age Position\\nJeffrey P. Bezos. Mr. Bezos founded Amazon.com in 1994 and has served as Executive Chair since July 2021. He has served as Chair of the Board since 1994 and served as Chief Executive Officer from May 1996 until July 2021, and as President from 1994 until June 1999 and again from October 2000 to July 2021.\\nAndrew R. Jassy. Mr. Jassy has served as President and Chief Executive Officer since July 2021, CEO Amazon Web Services from April 2016 until July 2021, and Senior Vice President, Amazon Web Services, from April 2006 until April 2016.\\nDouglas J. Herrington. Mr. Herrington has served as CEO Worldwide Amazon Stores since July 2022, Senior Vice President, North America Consumer from January 2015 to July 2022, and Senior Vice President, Consumables from May 2014 to December 2014.\\nBrian T. Olsavsky. Mr. Olsavsky has served as Senior Vice President and Chief Financial Officer since June 2015, Vice President, Finance for the Global Consumer Business from December 2011 to June 2015, and numerous financial leadership roles across Amazon with global responsibility since April 2002.\\nShelley L. Reynolds. Ms. Reynolds has served as Vice President, Worldwide Controller, and Principal Accounting Officer since April 2007.\\nAdam N. Selipsky. Mr. Selipsky has served as CEO Amazon Web Services since July 2021, Senior Vice President, Amazon Web Services from May 2021 until July 2021, President and CEO of Tableau Software from September 2016 until May 2021, and Vice President, Marketing, Sales and Support of Amazon Web Services from May 2005 to September 2016.\\nDavid A. Zapolsky. Mr. Zapolsky has served as Senior Vice President, General Counsel, and Secretary since May 2014, Vice President, General Counsel, and Secretary from September 2012 to May 2014, and as Vice President and Associate General Counsel for Litigation and Regulatory matters from April 2002 until September 2012.\\n5\\nJeffrey P. Bezos\\nAndrew R. Jassy\\nDouglas J. Herrington\\nBrian T. Olsavsky\\nShelley L. Reynolds\\nAdam N. Selipsky\\nDavid A. Zapolsky\\n59\\n55\\n56\\n59\\n58\\n56\\n59\\nExecutive Chair\\nPresident and Chief Executive Officer\\nCEO Worldwide Amazon Stores\\nSenior Vice President and Chief Financial Officer\\nVice President, Worldwide Controller, and Principal Accounting Officer\\nCEO Amazon Web Services\\nSenior Vice President, General Counsel, and Secretary', 'output': '<question_answer>David A. Zapolsky is the Senior Vice President, General Counsel and Secretary</question_answer>', 'page': '5'}\n"
     ]
    }
   ],
   "source": [
    "## 2. Download ground truth dataset\n",
    "\n",
    "import xmltodict\n",
    "url = 'https://d3q8adh3y5sxpk.cloudfront.net/rageval/qsdata_20.xml'\n",
    "\n",
    "# Send an HTTP GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:        \n",
    "    xml_data = xmltodict.parse(response.text)\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "qa_dataset = pd.DataFrame(xml_data['data']['records'])\n",
    "\n",
    "prompts = []\n",
    "for row in qa_dataset.itertuples():\n",
    "    item = {\n",
    "        'prompt': str(row[1]['Question']),\n",
    "        'context': str(row[1]['Context']),\n",
    "        'output': '<question_answer>' + str(row[1]['Answer']['question_answer']) + '</question_answer>',\n",
    "        'page': str(row[1]['Page'])\n",
    "    }\n",
    "    prompts.append(item)\n",
    "\n",
    "# example prompt\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae7ee402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Download context / Amazon annual report and create documents\n",
    "import numpy as np\n",
    "import pypdf\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [ \"https://d3q8adh3y5sxpk.cloudfront.net/rageval/AMZN-2023-10k.pdf\"]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)\n",
    "    \n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "196146f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: us-east-1\n",
      "creating collection\n",
      "new collection rag-eval created\n",
      "{'createCollectionDetail': {'arn': 'arn:aws:aoss:us-east-1:381492284087:collection/rhr29z1zirndid6qo9ag', 'createdDate': 1714945098738, 'id': 'rhr29z1zirndid6qo9ag', 'kmsKeyArn': 'auto', 'lastModifiedDate': 1714945098738, 'name': 'rag-eval', 'standbyReplicas': 'ENABLED', 'status': 'CREATING', 'type': 'VECTORSEARCH'}, 'ResponseMetadata': {'RequestId': 'c3d4d413-82de-41f5-8222-05929ecead4d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c3d4d413-82de-41f5-8222-05929ecead4d', 'date': 'Sun, 05 May 2024 21:38:18 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '300', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n",
      "create knowledge base\n",
      "using the following S3 bucket as source: sagemaker-us-east-1-381492284087\n",
      "Index does not exist, create the index\n",
      "knowledge_base_name: bedrock-kb-eval\n",
      "knowledge_base_description: bedrock-kb-eval\n",
      "knowledge_base_resource_role_arn: arn:aws:iam::381492284087:role/AmazonBedrockExecutionRoleForKnowledgeBase_rageval232\n",
      "embeddingModelArn: arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1\n",
      "collection_arn: arn:aws:aoss:us-east-1:381492284087:collection/rhr29z1zirndid6qo9ag\n",
      "index_name: bedrock-kb-eval\n",
      "vector_field: kb_vector\n",
      "text_field: kb_text\n",
      "bedrock_metadata_field: bedrock\n",
      "MTcxNDk0NTY0MTM5MF8x\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_source_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=343'>344</a>\u001b[0m         data_source_id \u001b[39m=\u001b[39m data_source[\u001b[39m'\u001b[39m\u001b[39mdataSourceId\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=345'>346</a>\u001b[0m \u001b[39m# configure data_source\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=346'>347</a>\u001b[0m data_source_config \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=347'>348</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mknowledgeBaseId\u001b[39m\u001b[39m\"\u001b[39m: knowledge_base_id,\n\u001b[0;32m--> <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=348'>349</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: data_source_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=349'>350</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m\"\u001b[39m: data_source_name,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=350'>351</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdataSourceConfiguration\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=351'>352</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mS3\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=352'>353</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39ms3Configuration\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=353'>354</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbucketArn\u001b[39m\u001b[39m\"\u001b[39m: knowledge_base_bucket_arn,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=354'>355</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minclusionPrefixes\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=355'>356</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mkb/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=356'>357</a>\u001b[0m             ]\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=357'>358</a>\u001b[0m         }\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=358'>359</a>\u001b[0m     },\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=359'>360</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mvectorIngestionConfiguration\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=360'>361</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mchunkingConfiguration\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=361'>362</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchunkingStrategy\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mFIXED_SIZE\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=362'>363</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfixedSizeChunkingConfiguration\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=363'>364</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mmaxTokens\u001b[39m\u001b[39m\"\u001b[39m: max_token_chunk,\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=364'>365</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39moverlapPercentage\u001b[39m\u001b[39m\"\u001b[39m: overlap\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=365'>366</a>\u001b[0m             }\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=366'>367</a>\u001b[0m         }\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=367'>368</a>\u001b[0m     }\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=368'>369</a>\u001b[0m }\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=370'>371</a>\u001b[0m ds_response \u001b[39m=\u001b[39m bedrock_agent_client\u001b[39m.\u001b[39mcreate_data_source(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata_source_config)\n\u001b[1;32m    <a href='vscode-notebook-cell://t3kkc7uwcvculn5.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-rag-system-eval/RAG-System-Eval-DeepDive-mlflow.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=371'>372</a>\u001b[0m data_source_id \u001b[39m=\u001b[39m ds_response[\u001b[39m'\u001b[39m\u001b[39mdataSource\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdataSourceId\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_source_name' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Create or get vector database (OpenSearch Serverless)\n",
    "## Provisioning below resources requires signficant access. \n",
    "## Thus if you do not have admin access or already have an existing OpenSearch Serverless endpoint and Bedrock Knowledge Base, you can just update the corresponding variable in dev-mlflow.env\n",
    "\n",
    "import time\n",
    "import os\n",
    "import dotenv\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "host = os.environ['OPENSEARCH_COLLECTION']  # serverless collection endpoint, without https://\n",
    "#print(f\"host: {host}\")\n",
    "region = os.environ['REGION']  # e.g. us-east-1\n",
    "print(f'region: {region}')\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "## 4a. create vectordatabase if it does not exist yet\n",
    "if host == 'XXX':\n",
    "    print('creating collection')\n",
    "    vector_store_name = 'rag-eval'\n",
    "    encryption_policy_name = \"rag-eval-ep\"\n",
    "    network_policy_name = \"rag-eval-np\"\n",
    "    access_policy_name = 'rag-eval-ap'\n",
    "    identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "    aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "    security_policy = aoss_client.create_security_policy(\n",
    "        name = encryption_policy_name,\n",
    "        policy = json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type = 'encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name = network_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type = 'network'\n",
    "    )\n",
    "\n",
    "    collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "    while True:\n",
    "        status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "        if status in ('ACTIVE', 'FAILED'): \n",
    "            print(f'new collection {vector_store_name} created')\n",
    "            break\n",
    "        time.sleep(10)\n",
    "\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name = access_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type = 'data'\n",
    "    )\n",
    "    print(collection)\n",
    "    host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'\n",
    "    host = host.split(\":\")[0].replace(\"'\",\"\")\n",
    "    # Write changes to .env file.\n",
    "    os.environ['OPENSEARCH_COLLECTION'] = host\n",
    "    dotenv.set_key('dev-mlflow.env', \"OPENSEARCH_COLLECTION\", os.environ[\"OPENSEARCH_COLLECTION\"])\n",
    "\n",
    "aospy_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")\n",
    "\n",
    "## 4b. create Bedrock KnowledgeBase if it does not exist yet\n",
    "\n",
    "if os.environ['BEDROCK_KNOWLEDGEBASE_ID'] == \"XXX\":\n",
    "    knowledge_base_id = \"\"\n",
    "    session = sagemaker.Session()\n",
    "\n",
    "    # please note this requires significant permissions, thus might have to be done outside of the notebook if your user role has insufficient access.\n",
    "    print('create knowledge base')\n",
    "\n",
    "    print(f'using the following S3 bucket as source: {bucket}')\n",
    "    prefix = \"bedrock-kb-eval\"\n",
    "\n",
    "    # upload source data to S3\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file('./data/AMZN-2023-10k.pdf', bucket, 'rageval/AMZN-2023-10k.pdf')\n",
    "\n",
    "    #session.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'rageval/AMZN-2023-10k.pdf')).upload_file('rageval/AMZN-2023-10k.pdf')\n",
    "\n",
    "    knowledge_base_name = prefix\n",
    "    knowledge_base_description = prefix\n",
    "    \n",
    "    knowledge_base_bucket_arn = f\"arn:aws:s3:::{bucket}\"\n",
    "    # parse collection id\n",
    "    collection_id = os.environ['OPENSEARCH_COLLECTION'].split(\".\")[0]\n",
    "    account_id = boto3.Session().client('sts').get_caller_identity()['Account']\n",
    "\n",
    "    collection_arn = f'arn:aws:aoss:{region}:{account_id}:collection/{collection_id}'\n",
    "    embeddingModelArn = f\"arn:aws:bedrock:{region}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "    # create IAM role\n",
    "    iam = boto3.client('iam')\n",
    "    policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\", \n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:PutObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{bucket}\",\n",
    "                    f\"arn:aws:s3:::{bucket}/*\"  \n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"BedrockAll\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            },\n",
    "            {\n",
    "                \"Sid\": \"PassRoleToBedrock\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"iam:PassRole\"\n",
    "                ],\n",
    "                \"Resource\": \"arn:aws:iam::*:role/*AmazonBedrock*\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"iam:PassedToService\": [\n",
    "                            \"bedrock.amazonaws.com\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"aoss:APIAccessAll\",\n",
    "                \"aoss:*\"\n",
    "            ],\n",
    "            \"Resource\": f'arn:aws:aoss:{region}:{account_id}:collection/{collection_id}'\n",
    "            }\n",
    "        ]\n",
    "        }\n",
    "    \n",
    "    rolename = f'AmazonBedrockExecutionRoleForKnowledgeBase_rageval{str(random_identifier)}'\n",
    "    role = iam.create_role(\n",
    "        RoleName=rolename,\n",
    "        AssumeRolePolicyDocument=json.dumps({\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Statement\": [\n",
    "                {\n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                    \"Service\": [\n",
    "                        \"bedrock.amazonaws.com\",\n",
    "                        \"opensearchservice.amazonaws.com\"\n",
    "                    ]\n",
    "                    },  \n",
    "                    \"Action\": \"sts:AssumeRole\"\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    )\n",
    "\n",
    "    result = iam.put_role_policy(\n",
    "        RoleName=rolename,\n",
    "        PolicyName='AmazonBedrockExecutionRolePolicy',\n",
    "        PolicyDocument=json.dumps(policy_doc)\n",
    "    )\n",
    "    role_details = iam.get_role(RoleName=rolename)\n",
    "\n",
    "    knowledge_base_resource_role_arn = role_details['Role']['Arn']\n",
    "\n",
    "    # create OpenSearch index\n",
    "    index_name = prefix\n",
    "    vector_field = \"kb_vector\"\n",
    "    text_field = \"kb_text\"\n",
    "    bedrock_metadata_field = \"bedrock\"\n",
    "    vector_size = 1536\n",
    "\n",
    "    index_found = False\n",
    "    try:\n",
    "        client.indices.get(index=index_name)\n",
    "        index_found = True\n",
    "    except:\n",
    "        print(\"Index does not exist, create the index\")\n",
    "\n",
    "    #create a new index\n",
    "    index_body = {\n",
    "        \"settings\": {\n",
    "            \"index.knn\": True\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "        f\"{vector_field}\": { \"type\": \"knn_vector\", \"dimension\": vector_size, \"method\": {\"engine\": \"faiss\", \"space_type\": \"l2\", \"name\": \"hnsw\", \"parameters\": {}   } },\n",
    "        f\"{text_field}\": { \"type\": \"text\" },\n",
    "        f\"{bedrock_metadata_field}\": { \"type\": \"text\", \"index\": False }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "\n",
    "    if index_found == False:\n",
    "        # wait 60 seconds for index creation to complete\n",
    "        time.sleep(60)\n",
    "        aospy_client.indices.create(index=index_name,body=index_body)\n",
    "\n",
    "    # wait 90 seconds for index creation to complete\n",
    "    time.sleep(90)\n",
    "\n",
    "    aospy_client.indices.get(index=index_name)\n",
    "\n",
    "    print(f'knowledge_base_name: {knowledge_base_name}')\n",
    "    print(f'knowledge_base_description: {knowledge_base_description}')\n",
    "    print(f'knowledge_base_resource_role_arn: {knowledge_base_resource_role_arn}')\n",
    "    print(f'embeddingModelArn: {embeddingModelArn}')\n",
    "    print(f'collection_arn: {collection_arn}')\n",
    "    print(f'index_name: {index_name}')\n",
    "    print(f'vector_field: {vector_field}')\n",
    "    print(f'text_field: {text_field}')\n",
    "    print(f'bedrock_metadata_field: {bedrock_metadata_field}')\n",
    "\n",
    "    # add Bedrock KB role to OpenSearch Serverless access policy\n",
    "    access_policy_get = aoss_client.get_access_policy(\n",
    "            name = access_policy_name,\n",
    "            type = 'data')\n",
    "\n",
    "    access_policy_version = access_policy_get['accessPolicyDetail']['policyVersion']\n",
    "    print(access_policy_version)\n",
    "\n",
    "    access_policy = aoss_client.update_access_policy(\n",
    "        name = access_policy_name,\n",
    "        policyVersion = access_policy_version,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity,knowledge_base_resource_role_arn],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type = 'data'\n",
    "    )\n",
    "\n",
    "    # create knowledge base\n",
    "    knowledge_base_config = {\n",
    "        \"name\": knowledge_base_name,\n",
    "        \"description\": knowledge_base_description,\n",
    "        \"roleArn\":knowledge_base_resource_role_arn,\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            \"type\": 'VECTOR',\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        \"storageConfiguration\": {\n",
    "            \"type\": 'OPENSEARCH_SERVERLESS',\n",
    "            \"opensearchServerlessConfiguration\": {\n",
    "                \"collectionArn\": collection_arn,\n",
    "                \"vectorIndexName\": index_name,\n",
    "                \"fieldMapping\": {\n",
    "                    \"vectorField\": vector_field,\n",
    "                    \"textField\": text_field,\n",
    "                    \"metadataField\": bedrock_metadata_field\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    bedrock_agent_client = boto3.client('bedrock-agent', region_name = region)\n",
    "    if knowledge_base_id == \"\":\n",
    "        response = bedrock_agent_client.create_knowledge_base(**knowledge_base_config)\n",
    "        knowledge_base_id = response['knowledgeBase']['knowledgeBaseId']                      \n",
    "\n",
    "    data_source_id = \"\"\n",
    "    max_token_chunk = 300\n",
    "    overlap = 10\n",
    "\n",
    "    response = bedrock_agent_client.list_data_sources(\n",
    "        knowledgeBaseId=knowledge_base_id\n",
    "    )\n",
    "    for data_source in response['dataSourceSummaries']:\n",
    "        if data_source['knowledgeBaseId'] == knowledge_base_id:\n",
    "            data_source_id = data_source['dataSourceId']\n",
    "\n",
    "    # configure data_source\n",
    "    data_source_name = knowledge_base_name\n",
    "    data_source_config = {\n",
    "        \"knowledgeBaseId\": knowledge_base_id,\n",
    "        \"name\": data_source_name,\n",
    "        \"description\": data_source_name,\n",
    "        \"dataSourceConfiguration\": {\n",
    "            \"type\": 'S3',\n",
    "            \"s3Configuration\": {\n",
    "                \"bucketArn\": knowledge_base_bucket_arn,\n",
    "                \"inclusionPrefixes\": [\n",
    "                    \"rageval/\",\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"vectorIngestionConfiguration\": {\n",
    "            \"chunkingConfiguration\": {\n",
    "                \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "                \"fixedSizeChunkingConfiguration\": {\n",
    "                    \"maxTokens\": max_token_chunk,\n",
    "                    \"overlapPercentage\": overlap\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    ds_response = bedrock_agent_client.create_data_source(**data_source_config)\n",
    "    data_source_id = ds_response['dataSource']['dataSourceId']\n",
    "    \n",
    "    response = bedrock_agent_client.list_data_sources( knowledgeBaseId=knowledge_base_id)\n",
    "    for data_source in response['dataSourceSummaries']:\n",
    "        if data_source['knowledgeBaseId'] == knowledge_base_id:\n",
    "            data_source_id = data_source['dataSourceId']\n",
    "\n",
    "    print(f\"data_source_id: {data_source_id}\")\n",
    "\n",
    "    ingestion_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId=knowledge_base_id,dataSourceId=data_source_id)\n",
    "    print(f\"ingestion response: {ingestion_response}\")\n",
    "    \n",
    "    ingestion_job = ingestion_response['ingestionJob']['ingestionJobId']\n",
    "    print(f\"ingestionJobId: {ingestion_job}\")\n",
    "    status = 'IN_PROGRESS'\n",
    "    response = {}\n",
    "    \n",
    "    while status == 'IN_PROGRESS':\n",
    "        response = bedrock_agent_client.get_ingestion_job(\n",
    "            knowledgeBaseId=knowledge_base_id,\n",
    "            dataSourceId=data_source_id,\n",
    "            ingestionJobId=ingestion_job\n",
    "        )\n",
    "        \n",
    "        status = response['ingestionJob']['status']\n",
    "        time.sleep(5)\n",
    "    \n",
    "    print(response['ingestionJob']['statistics'])\n",
    "\n",
    "    # Write changes to .env file.\n",
    "    os.environ['BEDROCK_KNOWLEDGEBASE_ID'] = knowledge_base_id\n",
    "    dotenv.set_key('dev-rageval.env', \"BEDROCK_KNOWLEDGEBASE_ID\", os.environ[\"BEDROCK_KNOWLEDGEBASE_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5292f998-cb7d-408f-8942-f5bba5700a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Create and save prompt templates for evaluation\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "### Claude prompt templates\n",
    "# Template 1: Long prompt with XML tags context chunks, and step by step instructions, and reiterating how the response should be provided in the end.\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "        You will be provided with a report extract between <report></report> XML tags, please read it and analyse the content.\n",
    "        Please answer the following question: \n",
    "        {question} \n",
    "        \n",
    "        The answer must only be based on the information from the report.\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return \"There is not enough information available to answer this question\" inside the XML tags.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "PROMPT_CLAUDE_1 = PromptTemplate(\n",
    "    template=prompt_template_claude_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# Template 2: InContextLearning prompt with example interaction between Human and Assistant\n",
    "prompt_template_claude_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest research assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"There is not enough information available to answer this question\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"There is not enough information available to answer this question\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Return the answer inside <question_answer></question_answer> XML tags. \n",
    "\n",
    "        Assistant:\n",
    "        I will provide the answer inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        Human:\n",
    "        Now read and analyse the context provided in <report></report> XML tags carefully.\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "        Then answer the below question:\n",
    "        {question}\n",
    "\n",
    "        Assistant:<question_answer>\n",
    "        \"\"\"\n",
    "\n",
    "PROMPT_CLAUDE_2 = PromptTemplate(\n",
    "    template=prompt_template_claude_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "### Llama2 prompt templates\n",
    "prompt_template_llama_1 = \"\"\"\n",
    "        [INST] Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags. [/INST]\n",
    "        \"\"\"\n",
    "PROMPT_LLAMA_1 = PromptTemplate(\n",
    "    template=prompt_template_llama_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_llama_2 = \"\"\"\n",
    "        [INST]\n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "        [/INST]\n",
    "\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        [INST]\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "        [/INST]\n",
    "\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        [INST]\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "        [/INST]\n",
    "\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "PROMPT_LLAMA_2 = PromptTemplate(\n",
    "    template=prompt_template_llama_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "\n",
    "### Cohere Command prompt templates\n",
    "prompt_template_command_1 = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "PROMPT_COMMAND_1 = PromptTemplate(\n",
    "    template=prompt_template_command_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_command_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "\n",
    "        Assistant:\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "PROMPT_COMMAND_2 = PromptTemplate(\n",
    "    template=prompt_template_command_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# generic prompt template for all LLMs\n",
    "generic_rag_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompttemplates = [\n",
    "    {'template_name': 'generic_rag_template', 'template': generic_rag_template},\n",
    "    {'template_name': 'prompt_template_claude_1', 'template': PROMPT_CLAUDE_1},\n",
    "    {'template_name': 'prompt_template_claude_2', 'template': PROMPT_CLAUDE_2},\n",
    "    {'template_name': 'prompt_template_command_1', 'template': PROMPT_COMMAND_1},\n",
    "    {'template_name': 'prompt_template_command_2', 'template': PROMPT_COMMAND_2},\n",
    "    {'template_name': 'prompt_template_llama_1', 'template': PROMPT_LLAMA_1},\n",
    "    {'template_name': 'prompt_template_llama_2', 'template': PROMPT_LLAMA_2},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30244609",
   "metadata": {},
   "source": [
    "## 6) Create 2 helper functions:\n",
    "### 6a. create_rag to create RAG systems for evaluation\n",
    "### 6b. run_ragas_eval to evaluate RAG system with RAGAS and LlamaIndex evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88ca290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<botocore.credentials.RefreshableCredentials object at 0x7f1cd7b54c10>\n"
     ]
    }
   ],
   "source": [
    "# 6a. helper function to create RAG systems for evaluation\n",
    "\n",
    "import random\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "import time\n",
    "\n",
    "# # LangChain requires AWS4Auth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "def get_aws4_auth():\n",
    "    region = os.environ.get(\"Region\", os.environ[\"REGION\"])\n",
    "    service = \"aoss\"\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    print(credentials)\n",
    "    return AWS4Auth(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        region,\n",
    "        service,\n",
    "        session_token=credentials.token,\n",
    "    )\n",
    "aws4_auth = get_aws4_auth()\n",
    "\n",
    "\n",
    "def create_rag(rag_system_details, debug):\n",
    "    from langchain_core.globals import set_debug\n",
    "    # if you run into any error, set langchain debug to true\n",
    "    if debug == True:\n",
    "        set_debug(True)\n",
    "    else:\n",
    "        set_debug(False)\n",
    "        \n",
    "    existing_vector_store = rag_system_details[\"vector_store\"]\n",
    "    kb_id = rag_system_details[\"kb_id\"]\n",
    "    llm = rag_system_details[\"llm\"]\n",
    "    aospy_client = rag_system_details[\"aospy_client\"]\n",
    "    embedding_model = rag_system_details[\"embedding_model\"]\n",
    "    embedding_model_name = rag_system_details[\"embedding_model_name\"]\n",
    "    splitter_name = rag_system_details[\"splitter_name\"]\n",
    "    text_chunks = rag_system_details[\"text_chunks\"]\n",
    "    index_name = rag_system_details[\"index_name\"]\n",
    "    index_dimension = rag_system_details[\"index_dimension\"]\n",
    "    prompt_template_name = rag_system_details[\"prompt_template_name\"]\n",
    "    prompt_template = rag_system_details[\"prompt_template\"]\n",
    "    chain_type= rag_system_details[\"chain_type\"]\n",
    "    search_type= rag_system_details[\"search_type\"]\n",
    "    retriever_k = rag_system_details[\"retriever_k\"]\n",
    "    score_threshold = rag_system_details[\"score_threshold\"]\n",
    "    fetch_k = rag_system_details[\"fetch_k\"]\n",
    "    lambda_mult = rag_system_details[\"lambda_mult\"]\n",
    "    vector_store = \"\"\n",
    "    if existing_vector_store == \"\" and kb_id == \"\":\n",
    "        # create index\n",
    "        knn_index = {\n",
    "            \"settings\": {\n",
    "                \"index.knn\": True\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"vector_field\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"method\": {\n",
    "                        \"engine\": \"faiss\",\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"space_type\": \"l2\"\n",
    "                        },\n",
    "                        \"dimension\": index_dimension,\n",
    "                        \"store\": True\n",
    "                    },\n",
    "                    \"text\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"store\": True\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            print('trying to create new index')\n",
    "            aospy_client.indices.delete(index=index_name)\n",
    "            aospy_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "            aospy_client.indices.get(index=index_name)\n",
    "        except:\n",
    "            #print(f'Index {index_name} not found. Creating index on OpenSearch.')\n",
    "            aospy_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "            aospy_client.indices.get(index=index_name)\n",
    "\n",
    "        # generate embeddings\n",
    "        full_opensearch_endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "        vector_store = OpenSearchVectorSearch.from_documents(\n",
    "                    index_name = index_name,\n",
    "                    documents = text_chunks,\n",
    "                    embedding = embedding_model,\n",
    "                    opensearch_url=full_opensearch_endpoint,\n",
    "                    http_auth=auth,\n",
    "                    use_ssl=True,\n",
    "                    verify_certs=True,\n",
    "                    connection_class=RequestsHttpConnection,\n",
    "                    timeout=60*3,\n",
    "                    bulk_size=4000,\n",
    "                    is_aoss=True\n",
    "                )\n",
    "\n",
    "        # add a wait timer, otherwise index is not ready in time for retrieval\n",
    "        time.sleep(60)\n",
    "    if existing_vector_store != \"\" and kb_id == \"\":\n",
    "        print(f'use existing vector store: {existing_vector_store}')\n",
    "        vector_store = existing_vector_store\n",
    "\n",
    "    if kb_id == \"\":\n",
    "        run_name=f'LLM_{llm.model_id}_embeddings{embedding_model_name}_split_{splitter_name}_template_{prompt_template_name}_search_{search_type}_chain_{chain_type}_k_{retriever_k}_{random_identifier}'\n",
    "        search_kwargs = {\n",
    "            \"retriever_k\": retriever_k\n",
    "        }\n",
    "        retriever = vector_store.as_retriever(search_type = search_type, search_kwargs=search_kwargs)\n",
    "    else:\n",
    "        # use Bedrock Knowledgebase retriever\n",
    "        print(f'using kb_id: {kb_id}')\n",
    "        run_name=f'LLM_{llm.model_id}_embeddings{embedding_model_name}_split_{splitter_name}_template_{prompt_template_name}_kb_id_{kb_id}_k_{retriever_k}_{random_identifier}'\n",
    "        retriever = AmazonKnowledgeBasesRetriever(\n",
    "                knowledge_base_id=kb_id,\n",
    "                retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": retriever_k}},\n",
    "\n",
    "            )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=chain_type,\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs = {\"prompt\": prompt_template}\n",
    "    )\n",
    "    \n",
    "    return run_name, vector_store, qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "73a13c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. helper function to evaluate RAG system with RAGAS\n",
    "\n",
    "import mlflow\n",
    "from mlflow.data.pandas_dataset import PandasDataset\n",
    "from datasets import Dataset\n",
    "import ragas\n",
    "#import tqdm as notebook_tqdm\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_relevancy,\n",
    "    answer_correctness,\n",
    "    #answer_similarity,\n",
    "    #context_relevancy\n",
    ")\n",
    "from langchain_core.globals import set_debug\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "inference_modifier = {\n",
    "    \"max_tokens_to_sample\": 545, # Claude-3 use “max_tokens” However Claud-2 requires “max_tokens_to_sample”\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 20,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "eval_llm_id = 'anthropic.claude-v2'\n",
    "\n",
    "ragas_eval_llm = langchain.llms.bedrock.Bedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=eval_llm_id,\n",
    "    model_kwargs=inference_modifier,\n",
    "    #streaming=True\n",
    ")\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "ragas_eval_embedding = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "\n",
    "def run_ragas_eval(rag_system_eval_details, rag_system_details, debug):\n",
    "    # if you run into any error, set langchain debug to true\n",
    "    if debug == True:\n",
    "        set_debug(True)\n",
    "    else:\n",
    "        set_debug(False)\n",
    "\n",
    "    mlflow_metrics_results = {}\n",
    "    experiment_name = rag_system_eval_details[\"experiment_name\"]\n",
    "    run_name = rag_system_eval_details[\"run_name\"]\n",
    "    qa_chain = rag_system_eval_details[\"qa_chain\"]\n",
    "    ground_truth = rag_system_eval_details[\"ground_truth\"]\n",
    "        \n",
    "    metrics = [\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        answer_correctness,\n",
    "        #answer_similarity,\n",
    "        #context_relevancy\n",
    "        \n",
    "    ]\n",
    "\n",
    "    basic_qa_ragas_dataset = []\n",
    "    basic_error_log = []\n",
    "    for item in ground_truth:\n",
    "        generated_answer_error = \"\"\n",
    "        retrieved_context_error = \"\"\n",
    "        reference_answer_error = \"\"\n",
    "        \n",
    "        result = qa_chain.invoke(item['prompt'])\n",
    "        #print(f'result: {result}')\n",
    "\n",
    "        question = item['prompt']\n",
    "        reference_answer = item['output']\n",
    "        generated_answer = result[\"result\"]\n",
    "\n",
    "        context_sequence = []\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            context_sequence.append(doc.page_content)\n",
    "        retrieved_context = context_sequence\n",
    "\n",
    "        if generated_answer is None or generated_answer.strip() == \"\" or generated_answer.replace(\" \", \"\") == \"<question_answer></question_answer>\":\n",
    "            generated_answer_error = f'no answer generated for question {question}'\n",
    "            \n",
    "        if retrieved_context is None or retrieved_context == \"\" or retrieved_context == \"[]\" or len(context_sequence) == 0 :\n",
    "            retrieved_context_error = f'no retrieved_context for question {question}'\n",
    "        \n",
    "        if reference_answer is None or reference_answer == \"\":\n",
    "            reference_answer_error = f'no reference_answer for question {question}'\n",
    "                \n",
    "        \n",
    "        if generated_answer_error != \"\" or retrieved_context_error != \"\" or reference_answer_error != \"\":\n",
    "            basic_error_log.append(\n",
    "                {\"question\" : question,\n",
    "                \"generated_answer_error\" : generated_answer_error,\n",
    "                \"retrieved_context_error\" : retrieved_context_error,\n",
    "                \"reference_answer_error\" : reference_answer_error,\n",
    "                \"value_error\": \"\"\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            \n",
    "            basic_qa_ragas_dataset.append(\n",
    "                {\"question\" : item['prompt'],\n",
    "                \"answer\" : result[\"result\"],\n",
    "                \"contexts\" : context_sequence,\n",
    "                \"ground_truth\" : item['output']\n",
    "                }\n",
    "            )\n",
    "\n",
    "    \n",
    "    #capture any errors in CSV for further review\n",
    "    if len(basic_error_log) != 0:\n",
    "        basic_error_log_df = pd.DataFrame(basic_error_log)\n",
    "        basic_error_log_df.to_csv('./mlflow_run_errorlog.csv',index=False)\n",
    "    \n",
    "    if len(basic_qa_ragas_dataset) != 0:\n",
    "        basic_qa_ragas_df = pd.DataFrame(basic_qa_ragas_dataset)\n",
    "        basic_qa_ragas_df.to_csv('./mlflow_run_predictions.csv',index=False)\n",
    "\n",
    "        # evaluate with RAGAS\n",
    "        basic_qa_ragas = Dataset.from_pandas(basic_qa_ragas_df)\n",
    "        \n",
    "        ragas_result = evaluate(basic_qa_ragas, metrics=metrics, llm=ragas_eval_llm, embeddings=ragas_eval_embedding,raise_exceptions=False ) #old: evaluate(basic_qa_ragas, metrics=metrics)\n",
    "        \n",
    "        ragas_evals_df = ragas_result.to_pandas()\n",
    "\n",
    "        params = {\n",
    "            \"run_name\": run_name,\n",
    "            \"llm_name\": rag_system_details[\"llm\"].model_id,\n",
    "            \"kb_id\": rag_system_details[\"kb_id\"],\n",
    "            \"splitter_name\": rag_system_details[\"splitter_name\"],\n",
    "            \"index_name\": rag_system_details[\"index_name\"],\n",
    "            \"index_dimension\": rag_system_details[\"index_dimension\"],\n",
    "            \"embedding_model_name\": rag_system_details[\"embedding_model_name\"],\n",
    "            \"prompt_template_name\": rag_system_details[\"prompt_template_name\"],\n",
    "            \"chain_type\": rag_system_details[\"chain_type\"],\n",
    "            \"search_type\": rag_system_details[\"search_type\"],\n",
    "            \"retriever_k\": rag_system_details[\"retriever_k\"],\n",
    "            \"score_threshold\": rag_system_details[\"score_threshold\"],\n",
    "            \"fetch_k\": rag_system_details[\"fetch_k\"],\n",
    "            \"lambda_mult\": rag_system_details[\"lambda_mult\"]\n",
    "        }\n",
    "        \n",
    "        print(f'ragas_faithfulness mean: {ragas_evals_df[\"faithfulness\"].mean()}')\n",
    "        print(f'ragas_answer_relevancy mean: {ragas_evals_df[\"answer_relevancy\"].mean()}')\n",
    "        print(f'ragas_context_recall mean:: {ragas_evals_df[\"context_recall\"].mean()}')\n",
    "        print(f'ragas_context_precision mean: {ragas_evals_df[\"context_precision\"].mean()}')\n",
    "        #print(f'ragas_context_relevancy mean: {ragas_evals_df[\"context_relevancy\"].mean()}')\n",
    "        #print(f'ragas_answer_similarity mean: {ragas_evals_df[\"answer_similarity\"].mean()}')\n",
    "        print(f'ragas_answer_correctness mean: {ragas_evals_df[\"answer_correctness\"].mean()}')\n",
    "\n",
    "        mlflow_metrics_results = {\n",
    "            \"ragas_faithfulness_mean\": ragas_evals_df[\"faithfulness\"].mean(),\n",
    "            \"ragas_answer_relevancy_mean\": ragas_evals_df[\"answer_relevancy\"].mean(),\n",
    "            \"ragas_context_recall_mean\": ragas_evals_df[\"context_recall\"].mean(),\n",
    "            \"ragas_context_precision_mean\": ragas_evals_df[\"context_precision\"].mean(),\n",
    "            # \"ragas_context_relevancy_mean\": ragas_evals_df[\"context_relevancy\"].mean(),\n",
    "            # \"ragas_answer_similarity_mean\": ragas_evals_df[\"answer_similarity\"].mean(),\n",
    "            \"ragas_answer_correctness_mean\": ragas_evals_df[\"answer_correctness\"].mean(),\n",
    "\n",
    "        }\n",
    "\n",
    "        if os.environ['MLFLOW_TRACKING_ENABLED'] == 'True':\n",
    "            llm_experiment = mlflow.set_experiment(experiment_name)\n",
    "            # Initiate the MLflow run context\n",
    "            with mlflow.start_run(run_name=run_name) as run:\n",
    "                # Log input dataset to MLflow Tracking as a JSON artifact.\n",
    "                mlflow_dataset = mlflow.data.from_pandas(basic_qa_ragas_df, source='./mlflow_run_predictions.csv')\n",
    "                mlflow.log_input(mlflow_dataset, context=\"rag-output\")\n",
    "\n",
    "                # Log evaluation metrics that were calculated\n",
    "                mlflow.log_metrics(mlflow_metrics_results)\n",
    "                \n",
    "                # Log RAGAS results to MLflow Tracking as a JSON artifact.\n",
    "                mlflow.log_table(data=ragas_evals_df, artifact_file=\"qa_ragas_eval_results.json\")\n",
    "\n",
    "                # Log parameters used for the RAG system\n",
    "                mlflow.log_params(params)\n",
    "\n",
    "    return mlflow_metrics_results, ragas_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "813ccb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c. helper function to analyse RAGAS results with an LLM\n",
    "\n",
    "# have an LLM analyse the results\n",
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def analze_ragas_result_chart(image_path, comparison):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_b64_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Variables for Bedrock API\n",
    "    modelId = 'anthropic.claude-3-haiku-20240307-v1:0' #'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    contentType = 'application/json'\n",
    "    accept = 'application/json'\n",
    "    prompt = \"\"\"The chart shows a comparison of the mean RAGAS metrics for faithfulness, answere relevance, answer correctness, context recall, and context precision. Follow the below instructions.\n",
    "                1. Analyse the chart carefully.\n",
    "                2. Determine which option performs best across all RAGAS metrics and include the result in the response in <best_overall></best_overall> xml tags.\n",
    "                3. Determine a use case and reason why someone might want to proceed with the option and why this option performed better compared to the other options ({comparison}). Include the reason in no more than 150 words in the response <best_overall_reason></best_overall_reason> xml tags.\n",
    "                3. Determine which option performs best if the metrics are prioritized in the followind order: \n",
    "                1)context recall,  2)context precision, 3)faithfulness, 4)answer relevance, 5)answer correctness. \n",
    "                Include the result in the response in <best_prioritized></best_prioritized> xml tags. \n",
    "                4. Determine a use case and reason why someone might want to proceed with this option and why this option performed better compared to the other options ({comparison}). Include the reason in no more than 150 words in the response <best_prioritized_reason></best_prioritized_reason> xml tags.\"\"\"\n",
    "\n",
    "    # Messages\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": \"image/png\",\n",
    "            \"data\": image_b64_string\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    # Body\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages\n",
    "    })\n",
    "\n",
    "    # Run Bedrock API\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "    modelId=modelId,\n",
    "    contentType=contentType,\n",
    "    accept=accept,\n",
    "    body=body\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "da7d2b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to create new index\n",
      "{'query': \"Who is Amazon's Senior Vice President and General Counsel?\", 'result': '<question_answer>David A. Zapolsky</question_answer>', 'source_documents': [Document(page_content='Executive Officers and Directors\\nThe following tables set forth certain information regarding our Executive Officers and Directors as of January 25, 2023:\\nInformation About Our Executive Officers\\nName Age Position\\nJeffrey P. Bezos 59 Executive Chair\\nAndrew R. Jassy 55 President and Chief Executive Officer\\nDouglas J. Herrington 56 CEO Worldwide Amazon Stores\\nBrian T. Olsavsky 59 Senior Vice President and Chief Financial Officer', metadata={'source': 'data/AMZN-2023-10k.pdf', 'page': 4}), Document(page_content='2021, and Senior Vice President, Amazon Web Services, from April 2006 until April 2016.\\nDouglas J. Herrington. Mr. Herrington has served as CEO Worldwide Amazon Stores since July 2022, Senior Vice President, North America Consumer\\nfrom January 2015 to July 2022, and Senior Vice President, Consumables from May 2014 to December 2014.\\nBrian T. Olsavsky. Mr. Olsavsky has served as Senior Vice President and Chief Financial Officer since June 2015, Vice President, Finance for the', metadata={'source': 'data/AMZN-2023-10k.pdf', 'page': 4}), Document(page_content='Brian T. Olsavsky 59 Senior Vice President and Chief Financial Officer\\nShelley L. Reynolds 58 Vice President, Worldwide Controller, and Principal Accounting Officer\\nAdam N. Selipsky 56 CEO Amazon Web Services\\nDavid A. Zapolsky 59 Senior Vice President, General Counsel, and Secretary\\nJeffrey P. Bezos. Mr. Bezos founded Amazon.com in 1994 and has served as Executive Chair since July 2021. He has served as Chair of the Board since', metadata={'source': 'data/AMZN-2023-10k.pdf', 'page': 4}), Document(page_content='Global Consumer Business from December 2011 to June 2015, and numerous financial leadership roles across Amazon with global responsibility since April\\n2002.\\nShelley L. Reynolds. Ms. Reynolds has served as Vice President, Worldwide Controller, and Principal Accounting Officer since April 2007.\\nAdam N. Selipsky. Mr. Selipsky has served as CEO Amazon Web Services since July 2021, Senior Vice President, Amazon Web Services from May', metadata={'source': 'data/AMZN-2023-10k.pdf', 'page': 4})]}\n"
     ]
    }
   ],
   "source": [
    "# 7. test a RAG system\n",
    "\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-chartextsplitter\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "query = \"Who is Amazon's Senior Vice President and General Counsel?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ad781",
   "metadata": {},
   "source": [
    "## 8) RAG system component evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d2279",
   "metadata": {},
   "source": [
    "### 8.1) Evaluate impact of different chunk sizes\n",
    "\n",
    "In this first eval, we keep everything else constant, and only change the chunk size.\n",
    "One word tends to be about 2-3 tokens, a paragraph is about 128 tokens.\n",
    "We test perform two experiments:\n",
    "\n",
    "In the first experiment we evaluate the best chunk size for a TokenTextSplitter. We test chunk sizes of 128, 256, 512, and 1536 tokens (which is the context window limit for the embeddings model).\n",
    "\n",
    "In the second experiment we evaluate the best chunk size for a RecursiveCharacterTextSplitter. Here the chunk size is defined as the maximum number of characters per chunk, not the number of tokens/words. Given that we are working with rather large documents such as 10k reports, we experiment with chunk sizes of 1000, 1500, 2000, and 2500 for LangChain's RecursiveCharacterTextSplitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3cd04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTextSplitter split documents in to 904 chunks.\n",
      "\n",
      "trying to create new index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522fc0bfb4744f74849cfa02cab40886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "Runner in Executor raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/output_parsers/json.py\", line 66, in parse_result\n",
      "    return parse_json_markdown(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/utils/json.py\", line 147, in parse_json_markdown\n",
      "    return _parse_json(json_str, parser=parser)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/utils/json.py\", line 160, in _parse_json\n",
      "    return parser(json_str)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/utils/json.py\", line 120, in parse_partial_json\n",
      "    return json.loads(s, strict=strict)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/json/__init__.py\", line 359, in loads\n",
      "    return cls(**kw).decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1 column 83 (char 82)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/llms/output_parser.py\", line 61, in aparse\n",
      "    output = super().parse(result)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py\", line 64, in parse\n",
      "    return super().parse(text)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/output_parsers/json.py\", line 72, in parse\n",
      "    return self.parse_result([Generation(text=text)])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/output_parsers/pydantic.py\", line 60, in parse_result\n",
      "    json_object = super().parse_result(result)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/output_parsers/json.py\", line 69, in parse_result\n",
      "    raise OutputParserException(msg, llm_output=text) from e\n",
      "langchain_core.exceptions.OutputParserException: Invalid json output: ```[\"The foreign exchange rate effect information is provided in the section titled \"Effect of Foreign Exchange Rates\" in the report.\"]```\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/asyncio/futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/metrics/base.py\", line 116, in ascore\n",
      "    raise e\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/metrics/base.py\", line 112, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/metrics/_faithfulness.py\", line 208, in _ascore\n",
      "    statements = await _statements_output_parser.aparse(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/llms/output_parser.py\", line 67, in aparse\n",
      "    output = await llm.generate(p_value)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/llms/base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/ragas/llms/base.py\", line 177, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 882, in _agenerate_helper\n",
      "    raise e\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 866, in _agenerate_helper\n",
      "    await self._agenerate(\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 1336, in _agenerate\n",
      "    await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"/home/sagemaker-user/.conda/envs/rag_eval/lib/python3.11/site-packages/langchain_community/llms/bedrock.py\", line 897, in _acall\n",
      "    raise ValueError(\"Streaming must be set to True for async operations. \")\n",
      "ValueError: Streaming must be set to True for async operations. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ragas_faithfulness mean: 0.8571428571428571\n",
      "ragas_answer_relevancy mean: 0.6786661247355793\n",
      "ragas_context_recall mean:: 0.6150793650533068\n",
      "ragas_context_precision mean: 0.48824996038943597\n",
      "ragas_answer_correctness mean: 0.6031203989390361\n"
     ]
    }
   ],
   "source": [
    "# 8.1.1.1 Chunk Size Eval with chunk_size=128, chunk_overlap=64\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=64) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter128\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter-128\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultchunk1,ragas_eval_resultchunk1df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d947b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1.2 Chunk Size Eval with chunk_size=256, chunk_overlap=64\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=256, chunk_overlap=64) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter256\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter-256\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultchunk2,ragas_eval_resultchunk2df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1.3 Chunk Size Eval with chunk_size=512, chunk_overlap=64\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=64) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter512\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter-512\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultchunk3,ragas_eval_resultchunk3df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1.4 Chunk Size Eval with chunk_size=1024, chunk_overlap=64\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=1536, chunk_overlap=64) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter1024\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter-1024\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultchunk4,ragas_eval_resultchunk4df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.1.1.5 Review chunk size test results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Extract metric names and mean values\n",
    "metric_names = list(ragas_eval_resultchunk1.keys())\n",
    "ragas_eval_result1_means = list(ragas_eval_resultchunk1.values())\n",
    "ragas_eval_result2_means = list(ragas_eval_resultchunk2.values())\n",
    "ragas_eval_result3_means = list(ragas_eval_resultchunk3.values())\n",
    "ragas_eval_result4_means = list(ragas_eval_resultchunk4.values())\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.15\n",
    "index = range(len(metric_names))\n",
    "\n",
    "bar1 = ax.bar(index, ragas_eval_result1_means, bar_width, label='Token128')\n",
    "bar2 = ax.bar([i + bar_width for i in index], ragas_eval_result2_means, bar_width, label='Token256')\n",
    "bar3 = ax.bar([i + 2*bar_width for i in index], ragas_eval_result3_means, bar_width, label='Token512')\n",
    "bar4 = ax.bar([i + 3*bar_width for i in index], ragas_eval_result4_means, bar_width, label='Token1024')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metrics for a token text splitter with different chunk sizes')\n",
    "ax.set_xticks([i + bar_width / 4 for i in index])\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "filename = './tmp/tokensplitter_chunksizes.png'\n",
    "plt.savefig(filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea094ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.1.6 LLM analysis\n",
    "image_path = \"./tmp/tokensplitter_chunksizes.png\"\n",
    "comparison = 'token splitter chunk size of 128 vs 256 vs 512 vs 1024'\n",
    "result = analze_ragas_result_chart(image_path, comparison)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28669de2",
   "metadata": {},
   "source": [
    "### 8.1.1.7 Review token splitter chunk size test results\n",
    "Token1024 performs the best overall across all RAGAS metrics, and is also performing best across our prioritized set of metrics. This indicates that a chunk size of 1024 is the most well-rounded option for a token text splitter and more importantly is also a good option for our use case in which the preservation of the original text's context and meaning is of utmost importance. The 1024 chunk size performed well in all context related metrics, and combined with its strong faithfulness scores, ensures that the output of the text splitter maintains the essential characteristics of the original text, making it an ideal choice for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0374d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2.1 Chunk Size Eval with chunk_size=1000, chunk_overlap=64\n",
    "\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=128)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "print(\"RecursiveCharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter1000\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter-1000\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultcharchunk1,ragas_eval_resultcharchunk1df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2.2 Chunk Size Eval with chunk_size=1500, chunk_overlap=64\n",
    "\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=128)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "print(\"RecursiveCharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter1500\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter-1500\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultcharchunk2,ragas_eval_resultcharchunk2df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10de7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2.3 Chunk Size Eval with chunk_size=2000, chunk_overlap=64\n",
    "\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=128)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "print(\"RecursiveCharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter2000\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter-2000\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultcharchunk3,ragas_eval_resultcharchunk3df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc963ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2.4 Chunk Size Eval with chunk_size=2500, chunk_overlap=64\n",
    "\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=128)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "print(\"RecursiveCharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter2500\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter-2500\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details,debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultcharchunk4,ragas_eval_resultcharchunk4df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.1.2.5 Review chunk size test results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Extract metric names and mean values\n",
    "metric_names = list(ragas_eval_resultcharchunk1.keys())\n",
    "ragas_eval_result1_means = list(ragas_eval_resultcharchunk1.values())\n",
    "ragas_eval_result2_means = list(ragas_eval_resultcharchunk2.values())\n",
    "ragas_eval_result3_means = list(ragas_eval_resultcharchunk3.values())\n",
    "ragas_eval_result4_means = list(ragas_eval_resultcharchunk4.values())\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.15\n",
    "index = range(len(metric_names))\n",
    "\n",
    "bar1 = ax.bar(index, ragas_eval_result1_means, bar_width, label='Char1000')\n",
    "bar2 = ax.bar([i + bar_width for i in index], ragas_eval_result2_means, bar_width, label='Char1500')\n",
    "bar3 = ax.bar([i + 2*bar_width for i in index], ragas_eval_result3_means, bar_width, label='Char2000')\n",
    "bar4 = ax.bar([i + 3*bar_width for i in index], ragas_eval_result4_means, bar_width, label='Char2500')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metrics for char splitter with different chunk sizes')\n",
    "ax.set_xticks([i + bar_width / 4 for i in index])\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "filename = './tmp/charsplitter_chunksizes.png'\n",
    "plt.savefig(filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1158d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1.2.6 LLM analysis\n",
    "image_path = \"./tmp/charsplitter_chunksizes.png\"\n",
    "comparison = 'recursive character text splitter chunk size of 1000 vs 1500 vs 2000 vs 2500'\n",
    "result = analze_ragas_result_chart(image_path, comparison)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cd4e2",
   "metadata": {},
   "source": [
    "### 8.1.2.7 Review recursive character text splitter chunk size test results\n",
    "Char2000 performs the best overall across all RAGAS metrics, and is also performing best across our prioritized set of metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0908ed",
   "metadata": {},
   "source": [
    "### 8.1.3 chunk size test results\n",
    "Token1536 and Char2000 performed best in this experiment, this is why we will compare the two in the next experiment below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb384a",
   "metadata": {},
   "source": [
    "### 8.2) Evaluate impact of different text splitters\n",
    "\n",
    "In this second eval, we keep everything else constant, and only change the splitter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2.1 RAG system with TokenTextSplitter\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=64) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter1024\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter-1024\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultsplitter1,ragas_eval_resultsplitter1df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2.2 RAG system with CharTextSplitter\n",
    "\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=64)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "print(\"CharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"RecursiveCharacterTextSplitter-2000\",\n",
    "    \"text_chunks\": char_text_list,\n",
    "    \"index_name\": \"rag-eval-charactertextplitter-2000\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'bedrock_embeddings',\n",
    "    \"embedding_model\": BedrockEmbeddings(client=bedrock_runtime),\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultsplitter2,ragas_eval_resultsplitter2df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.2.3 Review RecursiveCharacterTextSplitter vs TokenTextSplitter results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metric names and mean values\n",
    "metric_names = list(ragas_eval_resultsplitter1.keys())\n",
    "ragas_eval_result1_means = list(ragas_eval_resultsplitter1.values())\n",
    "ragas_eval_result2_means = list(ragas_eval_resultsplitter2.values())\n",
    "\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bar_width = 0.15\n",
    "index = range(len(metric_names))\n",
    "\n",
    "bar1 = ax.bar(index, ragas_eval_result1_means, bar_width, label='Token1024')\n",
    "bar2 = ax.bar([i + bar_width for i in index], ragas_eval_result2_means, bar_width, label='Char2000')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metrics between TokenTextSplitter and CharTextSplitter for different chunk sizes')\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure\n",
    "filename = './tmp/tokensplitter_vs_charsplitter.png'\n",
    "plt.savefig(filename)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2.4 LLM analysis\n",
    "image_path = \"./tmp/tokensplitter_vs_charsplitter.png\"\n",
    "comparison = 'recursive character text splitter with a chunk size of 2000 vs a token text splitter with a chunk size of 1024'\n",
    "result = analze_ragas_result_chart(image_path, comparison)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763e48a",
   "metadata": {},
   "source": [
    "### 8.2.5 Review RecursiveCharacterTextSplitter vs TokenTextSplitter results\n",
    "The TokenTextSplitter outperformed the RecursiveCharacterTextSplitter across the prioritized set of metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e0753",
   "metadata": {},
   "source": [
    "### 8.3) Evaluate impact of different embeddings models\n",
    "\n",
    "In this third eval, we proceed with the TokenTextSplitter, leave the  everything else constannt, and only change the embeddings models.\n",
    "Due to the context window length of the cohere model, we had to adjust the chunk size for the Cohere model to 350."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e789e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3.1 Create RAG system with amazon.titan-embed-text-v1\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=64) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_1024\",\n",
    "    \"index_dimension\": 1536,\n",
    "    \"embedding_model_name\": 'amazon.titan-embed-text-v1',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultembedding1,ragas_eval_resultembedding1df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3.2 Create RAG system with cohere.embed-multilingual-v3\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=350, chunk_overlap=16) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter350\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_350\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultembedding2,ragas_eval_resultembedding2df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3.3 Review Embedding model: amazon.titan-embed-text-v1 vs cohere.embed-multilingual-v3 results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metric names and mean values\n",
    "metric_names = list(ragas_eval_resultembedding1.keys())\n",
    "ragas_eval_result1_means = list(ragas_eval_resultembedding1.values())\n",
    "ragas_eval_result2_means = list(ragas_eval_resultembedding2.values())\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bar_width = 0.35\n",
    "index = range(len(metric_names))\n",
    "\n",
    "bar1 = ax.bar(index, ragas_eval_result1_means, bar_width, label='amazon.titan-embed-text-v1')\n",
    "bar2 = ax.bar([i + bar_width for i in index], ragas_eval_result2_means, bar_width, label='cohere.embed-multilingual-v3')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metrics between amazon.titan-embed-text-v1 and cohere.embed-multilingual-v3')\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "filename = './tmp/embeddingmodels_comparison.png'\n",
    "plt.savefig(filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3.4 LLM analysis\n",
    "image_path = \"./tmp/embeddingmodels_comparison.png\"\n",
    "comparison = 'Comparing the amazon.titan-embed-text-v1 embedding model with a chunk size of 1024 against a cohere.embed-multilingual-v3 embedding model with a chunk size of 350'\n",
    "result = analze_ragas_result_chart(image_path, comparison)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85354e91",
   "metadata": {},
   "source": [
    "### 8.3.5 Review Embedding model: amazon.titan-embed-text-v1 vs cohere.embed-multilingual-v3 results\n",
    "Here we can observe that for context recall, context precision, and faithfulness cohere.embed-multilingual-v3 performs better than amazon.titan-embed-text-v1 with a 350 token chunk size.\n",
    " \n",
    "As a result, we will proceed the next eval with cohere.embed-multilingual-v3 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47cbf2",
   "metadata": {},
   "source": [
    "### 8.4) Evaluate impact of different retriever search types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4.1 Create RAG system with Opensearch's “similarity” retriever\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=350, chunk_overlap=16) \n",
    "token_text_list = token_text_splitter.split_documents(documents)  \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter-350\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_350\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"similarity\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultretriever1, ragas_eval_resultretriever1df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4.2 Create RAG system with Opensearch's “mmr” retriever\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter-350\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_350\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultretriever2, ragas_eval_resultretriever2df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4.3 Create RAG system with Bedrock Knowledge Base retriever\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": \"\",\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": os.environ['BEDROCK_KNOWLEDGEBASE_ID'],\n",
    "    \"splitter_name\": \"TokenTextSplitter-350\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_350\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3', # Bedrock KB will use same embeddings model\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultretriever3, ragas_eval_resultretriever3df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4.4 Review “similarity” vs “mmr” vs Bedrock Knowledge Base retriever results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Extract metric names and mean values\n",
    "metric_names = list(ragas_eval_resultretriever1.keys())\n",
    "ragas_eval_result1_means = list(ragas_eval_resultretriever1.values())\n",
    "ragas_eval_result2_means = list(ragas_eval_resultretriever2.values())\n",
    "ragas_eval_result3_means = list(ragas_eval_resultretriever3.values())\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bar_width = 0.20\n",
    "index = range(len(metric_names))\n",
    "\n",
    "bar1 = ax.bar(index, ragas_eval_result1_means, bar_width, label='similarity')\n",
    "bar2 = ax.bar([i + bar_width for i in index], ragas_eval_result2_means, bar_width, label='mmr')\n",
    "bar3 = ax.bar([i + 2*bar_width for i in index], ragas_eval_result3_means, bar_width, label='bedrock_kb_retriever')\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metrics between “similarity” vs “mmr” vs Bedrock Knowledge Base retriever')\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure\n",
    "filename = './tmp/retriever_comparison.png'\n",
    "plt.savefig(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4.5 LLM analysis\n",
    "image_path = \"./tmp/retriever_comparison.png\"\n",
    "comparison = 'Comparing similarity with mmr and bedrock_kb retriever'\n",
    "result = analze_ragas_result_chart(image_path, comparison)\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55649b25",
   "metadata": {},
   "source": [
    "### 8.4.6 Review “similarity” vs “mmr” vs Bedrock Knowledge Base retriever results\n",
    "\n",
    "Bedrock's Knowledge Base Retriever with the Cohere embeddings model and default settings emerges as the best retriever.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d73f2",
   "metadata": {},
   "source": [
    "### 8.5) Evaluate impact of different prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4366349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5.1 Create RAG system with PROMPT_CLAUDE_1\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_1\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_1,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "\n",
    "ragas_eval_resultprompt1, ragas_eval_resultprompt1df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c597f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5.2 Create RAG system with PROMPT_CLAUDE_2\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"cohere.embed-multilingual-v3\"\n",
    ")\n",
    "\n",
    "rag_system_details = {\n",
    "    \"aospy_client\": aospy_client,\n",
    "    \"vector_store\": vector_store,\n",
    "    \"llm\": llm01,\n",
    "    \"kb_id\": \"\",\n",
    "    \"splitter_name\": \"TokenTextSplitter\",\n",
    "    \"text_chunks\": token_text_list,\n",
    "    \"index_name\": \"rag-eval-tokentextsplitter_1024\",\n",
    "    \"index_dimension\": 1024,\n",
    "    \"embedding_model_name\": 'cohere.embed-multilingual-v3',\n",
    "    \"embedding_model\": embedding_model,\n",
    "    \"prompt_template_name\": \"PROMPT_CLAUDE_2\",\n",
    "    \"prompt_template\": PROMPT_CLAUDE_2,\n",
    "    \"chain_type\": \"stuff\",\n",
    "    \"search_type\": \"mmr\", # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "    \"retriever_k\": 4, # Amount of documents to return (Default: 4)\n",
    "    \"score_threshold\": 0, # Minimum relevance threshold for similarity_score_threshold\n",
    "    \"fetch_k\": 20, # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "    \"lambda_mult\": 0.5, # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "         \n",
    "}\n",
    "run_name, vector_store, qa_chain = create_rag(rag_system_details, debug=False)\n",
    "\n",
    "rag_system_eval_details = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_name\": run_name,\n",
    "    \"qa_chain\": qa_chain,\n",
    "    \"ground_truth\": prompts\n",
    "}\n",
    "ragas_eval_resultprompt2, ragas_eval_resultprompt2df = run_ragas_eval(rag_system_eval_details, rag_system_details, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5.3 Review PROMPT_CLAUDE_1 vs PROMPT_CLAUDE_2 results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# Extract metric names and mean values\n",
    "metric_names = list(ragas_eval_resultprompt1.keys())\n",
    "ragas_eval_result1_means = list(ragas_eval_resultprompt1.values())\n",
    "ragas_eval_result2_means = list(ragas_eval_resultprompt2.values())\n",
    "\n",
    "# Create comparison chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bar_width = 0.35\n",
    "index = range(len(metric_names))\n",
    "\n",
    "bar1 = ax.bar(index, ragas_eval_result1_means, bar_width, label='PROMPT_CLAUDE_1')\n",
    "bar2 = ax.bar([i + bar_width for i in index], ragas_eval_result2_means, bar_width, label='PROMPT_CLAUDE_2')\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Mean Values')\n",
    "ax.set_title('Comparison of Metrics between PROMPT_CLAUDE_1 and PROMPT_CLAUDE_2 template')\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha='right')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the figure\n",
    "filename = './tmp/prompttemplate_comparison.png'\n",
    "plt.savefig(filename)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd3e507",
   "metadata": {},
   "source": [
    "### 8.5.4 Review PROMPT_CLAUDE_1 vs PROMPT_CLAUDE_2 results\n",
    "\n",
    "Overall comparable performance between the two prompt templates. Other factors such as prompt length/cost should be considered in addition to the above performance results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814cc8f6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In our benchmark here, a token text splitter with a chunk size of 350 and a chunk overlap of 16 worked well in combination with Cohere's embeddings model and Bedrock KnowledgeBase's retriever. Bedrock's Knowledge Base Retriever with Cohere embeddings model and default settings performed even slightly better across our prioritized RAGAS metrics compared to LangChain’s OpenSearch retriever with similarity or mmr search type on the given dataset.\n",
    "\n",
    "But more important than the specific benchmark results above, we established that there is no one size fits all approach and instead demonstrated a metric driven development approach to optimize the retrieval performance of a RAG system, which you can apply to your own data, RAG system, and use case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd4a21b",
   "metadata": {},
   "source": [
    "# Things to improve\n",
    "\n",
    "- Compare more advanced retrieval methods\n",
    "- Compare single and multi-language embeddings models\n",
    "- Compare more advanced prompt templates\n",
    "- Include cost dimension in analysis"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "rag-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
