{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8da33c-eeb6-4d72-a0fc-89f9fb6a19fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# RAG System Evaluation\n",
    "    \n",
    "This notebook is a follow up from the previous notebook in which we explored the overall evaluation approach and a RAG system's overall accuracy.\n",
    "\n",
    "This notebook we will take a closer look at specific RAG metrics and explore how different components and configurations can impact overall accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47020c-9266-4740-99c2-0d330b90d460",
   "metadata": {},
   "source": [
    "## Solution architecture\n",
    "<img src=\"https://d3q8adh3y5sxpk.cloudfront.net/meetingrecordings/modelevaluation/architecture.png\" alt=\"LLM selection process\" width=\"900\" height=\"550\">\n",
    "\n",
    "From the solution architecture, we will experiment with the below RAG components and evaluate the impact on several metric's relevant for RAG.\n",
    "\n",
    "- 1) Embedding model: amazon.titan-embed-text-v1 vs amazon.titan-e1t-medium \n",
    "- 2) Text Splitter: TokenTextSplitter vs CharacterTextSplitter\n",
    "- 3) Retriever: OpenSearch VectoreStoreRetriever search types “similarity” vs “mmr”\n",
    "- 4) Prompt Template: For each LLM we evaluate two different prompt templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4514e6",
   "metadata": {},
   "source": [
    "## RAG evaluation metrics\n",
    "\n",
    "This notebook explores the following metrics:\n",
    "\n",
    "Langsmith evaluators: \n",
    "-  a. \"cot_qa\"\n",
    "-  b. \"conciseness\"\n",
    "-  c. \"relevance\"\n",
    "\n",
    "Also review https://docs.smith.langchain.com/evaluation/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6dd86c-115c-49b9-b112-548edf312d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "%pip install --force-reinstall -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52a1c8-3b25-4bfa-bbff-5bbbd5de31c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel to ensure proper version of libraries is loaded\n",
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6646c5-8ccf-4e0b-8ea8-0501c379d551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip list | grep -E \"awscli|boto3|botocore|langchain|langsmith|plotly|tiktoken|nltk|python-dotenv|xmltodict|requests-aws4auth|pypdf|opensearch-py|sagemaker|nest-asyncio\"\n",
    "# also review requirements.txt for reference if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e938c8c-5e0b-483c-9efd-84c7c9d4e3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load environment variables \n",
    "import boto3\n",
    "import os\n",
    "import botocore\n",
    "from botocore.config import Config\n",
    "import langchain\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from typing import Dict\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# loading environment variables that are stored in local file dev.env\n",
    "load_dotenv(find_dotenv('dev-langsmith.env'),override=True)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "\n",
    "os.environ['OPENSEARCH_COLLECTION'] = os.getenv('OPENSEARCH_COLLECTION')\n",
    "os.environ['AWS_ACCESS_KEY'] = os.getenv('AWS_ACCESS_KEY')\n",
    "os.environ['AWS_SECRET_TOKEN'] = os.getenv('AWS_SECRET_TOKEN')\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING')\n",
    "os.environ[\"LANGCHAIN_TRACING\"]=\"false\"\n",
    "os.environ[\"LANGCHAIN_SESSION\"] = \"rag-eval\"\n",
    "\n",
    "# Initialize Bedrock runtime\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 8\n",
    "   }\n",
    ")\n",
    "bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1edb6-2102-48a2-bede-ffabd3caa462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize LLMs (Claude-V2, Cohere, LLama2)\n",
    "\n",
    "## 1a. Initialize Claude-v2\n",
    "llm01_inference_modifier = {\n",
    "    \"max_tokens_to_sample\": 545,\n",
    "    \"temperature\": 0,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "LLM_01_NAME= \"anthropic.claude-v2\"\n",
    "llm01 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_01_NAME,\n",
    "    model_kwargs=llm01_inference_modifier\n",
    ")\n",
    "\n",
    "## 1b. Initialize Cohere Command\n",
    "llm02_inference_modifier = { \n",
    "    \"max_tokens\": 545,\n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_02_NAME= \"cohere.command-text-v14\"\n",
    "llm02 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_02_NAME,\n",
    "    model_kwargs=llm02_inference_modifier\n",
    ")\n",
    "\n",
    "## 1c. Initialize Llama\n",
    "llm03_inference_modifier = { \n",
    "    \"max_gen_len\": 545,\n",
    "    \"top_p\": 0.9, \n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_03_NAME= \"meta.llama2-13b-chat-v1\"\n",
    "llm03 = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_03_NAME,\n",
    "    model_kwargs=llm03_inference_modifier\n",
    ")\n",
    "\n",
    "llms = [\n",
    "    llm01,\n",
    "    llm02,\n",
    "    llm03\n",
    "]\n",
    "\n",
    "## 1d. Initialize eval llm\n",
    "inference_modifier = { \n",
    "    \"max_gen_len\": 545,\n",
    "    \"top_p\": 0.9, \n",
    "    \"temperature\": 0,    \n",
    "}\n",
    "LLM_EVAL_NAME= \"meta.llama2-70b-chat-v1\"\n",
    "langchain_eval_llm = langchain.llms.bedrock.Bedrock( #create a Bedrock llm client\n",
    "    model_id=LLM_EVAL_NAME,\n",
    "    model_kwargs=inference_modifier\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60611855-58f1-4568-b250-88ed113262e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2a. download ground truth dataset\n",
    "import xmltodict\n",
    "url = 'https://d3q8adh3y5sxpk.cloudfront.net/rageval/qsdata_20.xml'\n",
    "\n",
    "# Send an HTTP GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:        \n",
    "    xml_data = xmltodict.parse(response.text)\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame\n",
    "qa_dataset = pd.DataFrame(xml_data['data']['records'])\n",
    "\n",
    "prompts = []\n",
    "for row in qa_dataset.itertuples():\n",
    "    item = {\n",
    "        'prompt': str(row[1]['Question']),\n",
    "        'context': str(row[1]['Context']),\n",
    "        'output': str(row[1]['Answer']['question_answer']),\n",
    "        'page': str(row[1]['Page'])\n",
    "    }\n",
    "    prompts.append(item)\n",
    "\n",
    "# example prompt\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c304e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. create ground truth dataset in langsmith\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"AMZN_groundtruthdata_20\"\n",
    "\n",
    "try:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Amazon 10k evaluation dataset\",\n",
    "    )\n",
    "    for prompt in prompts:\n",
    "        client.create_example(\n",
    "            inputs={\"input\": prompt['prompt']},\n",
    "            outputs={\"answer\": prompt['output']},\n",
    "            dataset_id=dataset.id,\n",
    "        )\n",
    "\n",
    "    print(\"Created a new dataset: \", dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ee402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create token_text_splitter and char_text_splitter for evaluation\n",
    "\n",
    "## 3a. download context / Amazon annual report\n",
    "import numpy as np\n",
    "import pypdf\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [ \"https://d3q8adh3y5sxpk.cloudfront.net/rageval/AMZN-2023-10k.pdf\"]\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    urlretrieve(url, file_path)\n",
    "    \n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "token_text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "char_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "token_text_list = token_text_splitter.split_documents(documents)\n",
    "char_text_list = char_text_splitter.split_documents(documents)\n",
    "    \n",
    "print(\"TokenTextSplitter split documents in to \" + str(len(token_text_list)) + \" chunks.\\n\")\n",
    "print(\"CharacterTextSplitter split documents in to \" + str(len(char_text_list)) + \" chunks.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196146f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. create vectors and store each document chunk in it's own index in vector database (OpenSearch Serverless)\n",
    "## 4a. connect to OpenSearchServerless\n",
    "import time\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "host = os.environ['OPENSEARCH_COLLECTION']  # serverless collection endpoint, without https://\n",
    "print(f\"host: {host}\")\n",
    "region = os.environ['REGION']  # e.g. us-east-1\n",
    "print(f'region: {region}')\n",
    "\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "## 4b. create vectordatabase if it does not exist yet\n",
    "if host == '':\n",
    "    print('creating collection')\n",
    "    vector_store_name = 'rag-eval'\n",
    "    encryption_policy_name = \"rag-eval-ep\"\n",
    "    network_policy_name = \"rag-eval-np\"\n",
    "    access_policy_name = 'rag-eval-ap'\n",
    "    identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "    aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "    security_policy = aoss_client.create_security_policy(\n",
    "        name = encryption_policy_name,\n",
    "        policy = json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type = 'encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name = network_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type = 'network'\n",
    "    )\n",
    "\n",
    "    collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "    while True:\n",
    "        status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "        if status in ('ACTIVE', 'FAILED'): \n",
    "            print(f'new collection {vector_store_name} created')\n",
    "            break\n",
    "        time.sleep(10)\n",
    "\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name = access_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type = 'data'\n",
    "    )\n",
    "\n",
    "    host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com:443'\n",
    "    host = host.split(\":\")[0]\n",
    "    print(f'new aoss host: {host}')\n",
    "\n",
    "aospy_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")\n",
    "print(f'aospy client:{aospy_client}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdaa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4c. Create index for CharacterTextSplitter in Amazon Opensearch Service \n",
    "\n",
    "# langchain version\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector_field\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"rag-eval-charactertextsplitter\"\n",
    "try:\n",
    "    aospy_client.indices.delete(index=index_name)\n",
    "    aospy_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "    aospy_client.indices.get(index=index_name)\n",
    "except:\n",
    "    print(f'Index {index_name} not found. Creating index on OpenSearch.')\n",
    "    aospy_client.indices.create(index=index_name,body=knn_index)\n",
    "    aospy_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51271938",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4d. Create index for TokenTextSplitter in Amazon Opensearch Service \n",
    "\n",
    "# langchain version\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector_field\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"rag-eval-tokentextsplitter\"\n",
    "try:\n",
    "    aospy_client.indices.delete(index=index_name)\n",
    "    aospy_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "    aospy_client.indices.get(index=index_name)\n",
    "except:\n",
    "    print(f'Index {index_name} not found. Creating index on OpenSearch.')\n",
    "    aospy_client.indices.create(index=index_name,body=knn_index)\n",
    "    aospy_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use Titan Embeddings Model to generate embeddings\n",
    "\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "\n",
    "# # LangChain requires AWS4Auth\n",
    "# from requests_aws4auth import AWS4Auth\n",
    "# def get_aws4_auth():\n",
    "#     region = os.environ.get(\"Region\", os.environ[\"REGION\"])\n",
    "#     service = \"aoss\"\n",
    "#     credentials = boto3.Session().get_credentials()\n",
    "#     return AWS4Auth(\n",
    "#         credentials.access_key,\n",
    "#         credentials.secret_key,\n",
    "#         region,\n",
    "#         service,\n",
    "#         session_token=credentials.token,\n",
    "#     )\n",
    "# aws4_auth = get_aws4_auth()\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d749746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5a. Use Titan Embeddings Model to generate embeddings for TokenTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings  \n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "full_opensearch_endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "index_name = \"rag-eval-tokentextsplitter\"  \n",
    "vectorstore_token = OpenSearchVectorSearch.from_documents(\n",
    "            index_name = index_name,\n",
    "            documents=token_text_list,\n",
    "            embedding=bedrock_embeddings,\n",
    "            opensearch_url=full_opensearch_endpoint,\n",
    "            http_auth=auth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            timeout=60*3,\n",
    "            bulk_size=1000,\n",
    "            is_aoss=True\n",
    "        )  \n",
    "retriever_token = vectorstore_token.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5b. Use Titan Embeddings Model to generate embeddings for CharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings  \n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "full_opensearch_endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "index_name = \"rag-eval-charactertextsplitter\"  \n",
    "vectorstore_character = OpenSearchVectorSearch.from_documents(\n",
    "            index_name = index_name,\n",
    "            documents=token_text_list,\n",
    "            embedding=bedrock_embeddings,\n",
    "            opensearch_url=full_opensearch_endpoint,\n",
    "            http_auth=auth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection,\n",
    "            timeout=60*3,\n",
    "            bulk_size=1000,\n",
    "            is_aoss=True\n",
    "        )  \n",
    "retriever_character = vectorstore_character.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292f998-cb7d-408f-8942-f5bba5700a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. create and save prompt templates for eval\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "### Claude prompt templates\n",
    "prompt_template_claude_1 = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "PROMPT_CLAUDE_1 = PromptTemplate(\n",
    "    template=prompt_template_claude_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_claude_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "\n",
    "        Assistant:\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "\n",
    "PROMPT_CLAUDE_2 = PromptTemplate(\n",
    "    template=prompt_template_claude_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "### Llama2 prompt templates\n",
    "prompt_template_llama_1 = \"\"\"\n",
    "        [INST] Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags. [/INST]\n",
    "        \"\"\"\n",
    "PROMPT_LLAMA_1 = PromptTemplate(\n",
    "    template=prompt_template_llama_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_llama_2 = \"\"\"\n",
    "        [INST]\n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "        [/INST]\n",
    "\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        [INST]\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "        [/INST]\n",
    "\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        [INST]\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "        [/INST]\n",
    "\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "PROMPT_LLAMA_2 = PromptTemplate(\n",
    "    template=prompt_template_llama_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "\n",
    "### Cohere Command prompt templates\n",
    "prompt_template_command_1 = \"\"\"\n",
    "        Human: Given report provided, please read it and analyse the content.\n",
    "        Please answer the following question: {question} basing the answer only on the information from the report\n",
    "        and return it inside <question_answer></question_answer> XML tags.\n",
    "\n",
    "        If a particular bit of information is not present, return an empty string.\n",
    "        Each returned answer should be concise, remove extra information if possible.\n",
    "        The report will be given between <report></report> XML tags.\n",
    "\n",
    "        <report>\n",
    "        {context}\n",
    "        </report>\n",
    "\n",
    "        Return the answer inside <question_answer></question_answer> XML tags.\n",
    "        Assistant:\"\"\"\n",
    "\n",
    "PROMPT_COMMAND_1 = PromptTemplate(\n",
    "    template=prompt_template_command_1, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "prompt_template_command_2 = \"\"\"\n",
    "        Human: \n",
    "        You are a helpful, respectful, and honest assistant, dedicated to providing valuable and accurate information.\n",
    "\n",
    "        Assistant:\n",
    "        Understood. I will provide information based on the context given, without relying on prior knowledge.\n",
    "\n",
    "        Human:\n",
    "        If you don't see answer in the context just reply \"not available\" in XML tags.\n",
    "\n",
    "        Assistant:\n",
    "        Noted. I will respond with \"not available\" if the information is not available in the context.\n",
    "\n",
    "        Human:\n",
    "        Now read this context and answer the question and return the answer inside <question_answer></question_answer> XML tags. \n",
    "        {context}\n",
    "\n",
    "        Assistant:\n",
    "        Based on the provided context above and information from the retriever source, I will provide the answer in  and return it inside <question_answer></question_answer> XML tags to the below question\n",
    "        {question}\n",
    "        \"\"\"\n",
    "PROMPT_COMMAND_2 = PromptTemplate(\n",
    "    template=prompt_template_command_2, input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# generic prompt template for all LLMs\n",
    "generic_rag_template = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompttemplates = [\n",
    "    {'template_name': 'generic_rag_template', 'template': generic_rag_template},\n",
    "    {'template_name': 'prompt_template_claude_1', 'template': PROMPT_CLAUDE_1},\n",
    "    {'template_name': 'prompt_template_claude_2', 'template': PROMPT_CLAUDE_2},\n",
    "    {'template_name': 'prompt_template_command_1', 'template': PROMPT_COMMAND_1},\n",
    "    {'template_name': 'prompt_template_command_2', 'template': PROMPT_COMMAND_2},\n",
    "    {'template_name': 'prompt_template_llama_1', 'template': PROMPT_LLAMA_1},\n",
    "    {'template_name': 'prompt_template_llama_2', 'template': PROMPT_LLAMA_2},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. create custom evaluators for LangSmith\n",
    "## 7a) Custom Evaluator with llama_index SemanticSimilarityEvaluator\n",
    "\n",
    "from typing import Optional\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "import nest_asyncio\n",
    "from llama_index.llms import Bedrock\n",
    "from llama_index.embeddings import BedrockEmbedding\n",
    "from llama_index import (\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "from llama_index.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.embeddings import SimilarityMode\n",
    "from llama_index import Document\n",
    "\n",
    "class LlamaIndexEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self, model: str = \"anthropic.claude-v2\"):\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.eval_llm = Bedrock(model=self.model,\n",
    "                    temperature=0,\n",
    "                    additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "        self.embed_model = BedrockEmbedding().from_credentials(\n",
    "            model_name='amazon.titan-embed-g1-text-02'\n",
    "        )\n",
    "\n",
    "        self.service_context_eval = ServiceContext.from_defaults(\n",
    "            llm=self.eval_llm, \n",
    "            embed_model=self.embed_model, \n",
    "        )\n",
    "        self.faithfulness_evaluator = FaithfulnessEvaluator(service_context=self.service_context_eval)\n",
    "        self.relevancy_evaluator = RelevancyEvaluator(service_context=self.service_context_eval)\n",
    "        self.similarity_threshold = 0.8\n",
    "        self.semantic_evaluator = SemanticSimilarityEvaluator(service_context=self.service_context_eval,\n",
    "                                                        similarity_mode=SimilarityMode.DEFAULT,\n",
    "                                                        similarity_threshold=self.similarity_threshold) # 0.8 default\n",
    "        self.correctness_evaluator = CorrectnessEvaluator(service_context=self.service_context_eval) # encountered parsing errors with this class\n",
    "\n",
    "\n",
    "    def evaluate_run(self, run, example: [Example]) -> EvaluationResult:\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        if example is None:\n",
    "            raise ValueError(\"Examples cannot be None\")\n",
    "        \n",
    "\n",
    "        print(f'example answer value: {str(example.outputs[\"answer\"])}')\n",
    "        print(f'example question value: {str(run.inputs[\"query\"])}')\n",
    "        print(f'run answer value: {str(run.outputs[\"result\"])}')\n",
    "\n",
    "        generated_answer=run.outputs[\"result\"]\n",
    "        reference_answer=example.outputs[\"answer\"]\n",
    "\n",
    "        nest_asyncio.apply()\n",
    "        semantic_results = self.semantic_evaluator.evaluate(\n",
    "            response=generated_answer,\n",
    "            reference=reference_answer\n",
    "        )\n",
    "\n",
    "        cur_result_dict = {\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"semantic_similarity\": semantic_results.passing,\n",
    "            \"semantic_similarity_threshold\": self.similarity_threshold,\n",
    "            \"semantic_similarity_score\": semantic_results.score\n",
    "        }\n",
    "        return EvaluationResult(key=\"Similarity\", score=semantic_results.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7b) Custom Evaluator with RAGAS framework for context_recall\n",
    "\n",
    "from typing import Optional\n",
    "from langsmith.evaluation import EvaluationResult, RunEvaluator\n",
    "from langsmith.schemas import Example, Run\n",
    "import nest_asyncio\n",
    "\n",
    "from datasets import Dataset\n",
    "import ragas\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    answer_relevancy,\n",
    ")\n",
    "\n",
    "class RagasContextRecallEvaluator(RunEvaluator):\n",
    "    \n",
    "    def __init__(self, model: str = \"anthropic.claude-v2\"):\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.eval_llm = Bedrock(model=self.model,\n",
    "                    temperature=0,\n",
    "                    additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "        self.embed_model = BedrockEmbedding().from_credentials(\n",
    "            model_name='amazon.titan-embed-g1-text-02'\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "    def evaluate_run(self, run, example: [Example]) -> EvaluationResult:\n",
    "        if run.inputs is None:\n",
    "            raise ValueError(\"Run inputs cannot be None\")\n",
    "        if run.outputs is None:\n",
    "            raise ValueError(\"Run outputs cannot be None\")\n",
    "        if example is None:\n",
    "            raise ValueError(\"Examples cannot be None\")\n",
    "        \n",
    "\n",
    "        print(f'example answer value: {str(example.outputs[\"answer\"])}')\n",
    "        print(f'example question value: {str(run.inputs[\"query\"])}')\n",
    "        print(f'run answer value: {str(run.outputs[\"result\"])}')\n",
    "\n",
    "        generated_answer=run.outputs[\"result\"]\n",
    "        reference_answer=example.outputs[\"answer\"]\n",
    "        question=run.inputs[\"query\"]\n",
    "\n",
    "\n",
    "        nest_asyncio.apply()\n",
    "        # list of metrics we're going to use\n",
    "        metrics = [\n",
    "            #faithfulness,\n",
    "            #answer_relevancy,\n",
    "            context_recall,\n",
    "            #context_precision,\n",
    "            # harmfulness,\n",
    "        ]\n",
    "\n",
    "        basic_qa_ragas_dataset = []\n",
    "        basic_qa_ragas_dataset.append(\n",
    "                {\"question\" :question,\n",
    "                \"answer\" : generated_answer,\n",
    "                \"contexts\" : [\"\"],\n",
    "                \"ground_truths\" : [reference_answer]\n",
    "                }\n",
    "            )\n",
    "        basic_qa_ragas_df = pd.DataFrame(basic_qa_ragas_dataset)\n",
    "        basic_qa_ragas_df = Dataset.from_pandas(basic_qa_ragas_df)\n",
    "\n",
    "        # evaluate\n",
    "        result = evaluate(basic_qa_ragas_df, metrics=metrics)\n",
    "        context_recall_results_df = result.to_pandas()\n",
    "\n",
    "        cur_result_dict = {\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"context_recall_score\": context_recall_results_df['context_recall'],\n",
    "        }\n",
    "        return EvaluationResult(key=\"ContextRecall\", score=float(cur_result_dict['context_recall_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42382ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "from langchain import hub\n",
    "from langchain import chat_models, prompts, smith\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import output_parser\n",
    "\n",
    "def langsmith_evaluate(test_name, dataset_name, tags, chain):\n",
    "    # Define the evaluators to apply\n",
    "    eval_config = smith.RunEvalConfig(\n",
    "        evaluators=[\n",
    "            \"cot_qa\",\n",
    "            smith.RunEvalConfig.LabeledCriteria(\"conciseness\"),\n",
    "            smith.RunEvalConfig.LabeledCriteria(\"relevance\")\n",
    "        ],\n",
    "        custom_evaluators=[\n",
    "                           LlamaIndexEvaluator(),\n",
    "                           RagasContextRecallEvaluator()\n",
    "                           ],\n",
    "        eval_llm=langchain_eval_llm\n",
    "    )\n",
    "\n",
    "    client = langsmith.Client()\n",
    "    chain_results = client.run_on_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        llm_or_chain_factory=chain,\n",
    "        evaluation=eval_config,\n",
    "        project_name=test_name,\n",
    "        concurrency_level=5,\n",
    "        verbose=True,\n",
    "        tags=tags\n",
    "    )\n",
    "    return chain_results\n",
    "\n",
    "vectorstores = [vectorstore_token, vectorstore_character]\n",
    "overall_results = []\n",
    "for llm in llms:\n",
    "    for prompttemplate in prompttemplates:\n",
    "        print(f'llm: {llm.model_id}')\n",
    "        print(f'prompt template: {prompttemplate[\"template_name\"]}')\n",
    "        \n",
    "        prompt = prompttemplate[\"template\"]\n",
    "        chain_type=\"stuff\"\n",
    "        search_type=\"similarity\" # alternative: \"mmr\", or \"similarity_score_threshold\" (Default: similarity)\n",
    "        retriever_k = 4 # Amount of documents to return (Default: 4)\n",
    "        score_threshold = 0 # Minimum relevance threshold for similarity_score_threshold\n",
    "        fetch_k = 20 # Amount of documents to pass to MMR algorithm (Default: 20)\n",
    "        lambda_mult = 0.5 # Diversity of results returned by MMR, 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
    "\n",
    "        \n",
    "\n",
    "        test_name=f'LLM_{llm.model_id}_vectorstore_token_template_{str(prompttemplate[\"template_name\"])}_search_{search_type}_chain_{chain_type}_k_{retriever_k}_21'\n",
    "        k_value = f'k_{retriever_k}'\n",
    "        chain_type_value = f'chain_{chain_type}'\n",
    "        tags = [llm.model_id, prompttemplate[\"template_name\"],search_type, chain_type_value, k_value]\n",
    "        print(test_name)\n",
    "\n",
    "        search_kwargs = {\n",
    "            \"retriever_k\": retriever_k\n",
    "        }\n",
    "\n",
    "        retriever = vectorstore_token.as_retriever(search_type = search_type, search_kwargs=search_kwargs)\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=llm,\n",
    "                chain_type=chain_type,\n",
    "                retriever=retriever,\n",
    "                chain_type_kwargs = {\"prompt\": prompt}\n",
    "            )\n",
    "\n",
    "        chain = qa_chain\n",
    "        dataset_name=\"AMZN_groundtruthdata_20\"\n",
    "\n",
    "        chain_results = langsmith_evaluate(test_name, dataset_name, tags, chain)\n",
    "        overall_results.append(chain_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea56a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA_INDEX EVAL\n",
    "\n",
    "## use results from LLMInformationExtraction.ipynb\n",
    "### query,llm,output,trainingoutput,context,trainingcontext,evaluationmetric,score,feedback\n",
    "predictions_df = pd.read_csv('eval_run_predictions.csv')\n",
    "print(f'column names: {predictions_df.columns}')\n",
    "print(f'no of rows: {predictions_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d363ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation directly with llama_index on an existing dataframe\n",
    "## Faithfulness: measure if the response from a query engine matches any source nodes\n",
    "## Relevancy: measure if the response and source nodes match the query\n",
    "## Correctness: assess the relevance and correctness of a generated answer against a reference answer\n",
    "## Semantic Similarity: evaluates the quality of a question answering system via semantic similarity\n",
    "\n",
    "from llama_index.llms import Bedrock\n",
    "from llama_index.embeddings import BedrockEmbedding\n",
    "from llama_index import (\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "from llama_index.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    SemanticSimilarityEvaluator\n",
    ")\n",
    "from llama_index.embeddings import SimilarityMode\n",
    "from llama_index import Document\n",
    "\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "#LLM_EVAL_NAME= \"meta.llama2-70b-chat-v1\"\n",
    "eval_llm = Bedrock(model=\"anthropic.claude-v2\",\n",
    "              #context_size=512,\n",
    "              temperature=0,\n",
    "              additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "embed_model = BedrockEmbedding().from_credentials(\n",
    "    model_name='amazon.titan-embed-g1-text-02'\n",
    ")\n",
    "\n",
    "service_context_eval = ServiceContext.from_defaults(\n",
    "    llm=eval_llm, \n",
    "    embed_model=embed_model, \n",
    ")\n",
    "\n",
    "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_eval)\n",
    "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_eval)\n",
    "similarity_threshold = 0.8\n",
    "semantic_evaluator = SemanticSimilarityEvaluator(service_context=service_context_eval,\n",
    "                                                 similarity_mode=SimilarityMode.DEFAULT,\n",
    "                                                 similarity_threshold=similarity_threshold) # 0.8 default\n",
    "correctness_evaluator = CorrectnessEvaluator(service_context=service_context_eval) # encountered parsing errors with this class\n",
    "\n",
    "def run_evals(qa_df):\n",
    "    results_list = []\n",
    "    for row in qa_df.itertuples(index=False):\n",
    "        question = row.query\n",
    "        reference_answer = row.trainingoutput\n",
    "        generated_answer = row.output\n",
    "        retrieved_context = row.context.replace('[]','')\n",
    "        retrieved_context = retrieved_context.split(\"/n\")\n",
    "        #print(f'retrieved context: {retrieved_context}')\n",
    "        #print(f'retrieved context type: {type(retrieved_context)}')\n",
    "\n",
    "        faithfulness = False\n",
    "        faithfulness_feedback  = 'not calculated'\n",
    "        faithfulness_score =  0.0\n",
    "        relevancy = False\n",
    "        relevancy_feedback =  'not calculated'\n",
    "        relevancy_score  =  0.0\n",
    "        correctness = False\n",
    "        correctness_feedback = 'not calculated'\n",
    "        correctness_score = 1.0\n",
    "        \n",
    "        if not(len(retrieved_context) == 0 or retrieved_context[0] == ''):\n",
    "\n",
    "            faithfulness_results = faithfulness_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                contexts=retrieved_context\n",
    "                )\n",
    "            \n",
    "            relevancy_results = relevancy_evaluator.evaluate(\n",
    "                query=question,\n",
    "                response=generated_answer,\n",
    "                contexts=retrieved_context\n",
    "                )\n",
    "            faithfulness = faithfulness_results.passing\n",
    "            faithfulness_feedback  = faithfulness_results.feedback\n",
    "            faithfulness_score =  faithfulness_results.score\n",
    "            relevancy = relevancy_results.passing\n",
    "            relevancy_feedback =  relevancy_results.feedback\n",
    "            relevancy_score  =  relevancy_results.score\n",
    "            \n",
    "        semantic_results = semantic_evaluator.evaluate(\n",
    "            response=generated_answer,\n",
    "            reference=reference_answer\n",
    "        )\n",
    "\n",
    "        # correctness_results = correctness_evaluator.evaluate(\n",
    "        #     query=question,\n",
    "        #     response=generated_answer,\n",
    "        #     reference=reference_answer\n",
    "        # )\n",
    "\n",
    "        # correctness= correctness_results.passing\n",
    "        # correctness_feedback= correctness_results.feedback\n",
    "        # correctness_score= correctness_results.score\n",
    "\n",
    "        cur_result_dict = {\n",
    "            \"query\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"correctness\": correctness,\n",
    "            \"correctness_feedback\": correctness_feedback,\n",
    "            \"correctness_score\": correctness_score,\n",
    "            \"semantic_similarity\": semantic_results.passing,\n",
    "            \"semantic_similarity_threshold\": similarity_threshold,\n",
    "            \"semantic_similarity_score\": semantic_results.score,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"faithfulness_feedback\": faithfulness_feedback,\n",
    "            \"faithfulness_score\": faithfulness_score,\n",
    "            \"relevancy\": relevancy,\n",
    "            \"relevancy_feedback\": relevancy_feedback,\n",
    "            \"relevancy_score\": relevancy_score\n",
    "        }\n",
    "        results_list.append(cur_result_dict)\n",
    "    evals_df = pd.DataFrame(results_list)\n",
    "    return evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea6e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df = run_evals(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa43dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify results \n",
    "\n",
    "# mean in each dimension\n",
    "print(f'faithfulness mean: {evals_df[\"faithfulness_score\"].mean()}')\n",
    "print(f'relevancy mean: {evals_df[\"relevancy_score\"].mean()}')\n",
    "print(f'semantic mean: {evals_df[\"semantic_similarity_score\"].mean()}')\n",
    "print(f'correctness mean: {evals_df[\"correctness_score\"].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST LLAMA_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ee3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "!mkdir -p ./data\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "urls = [\n",
    "    'https://d3q8adh3y5sxpk.cloudfront.net/rageval/AMZN-2023-10k.pdf',\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AMZN-2023-10k.pdf',\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    get_response_synthesizer,\n",
    "    set_global_service_context\n",
    ")\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b426d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Bedrock\n",
    "from llama_index.embeddings import BedrockEmbedding\n",
    "\n",
    "model_kwargs_claude = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_k\": 10,\n",
    "    \"max_tokens_to_sample\": 512\n",
    "}\n",
    "\n",
    "llm = Bedrock(model=\"anthropic.claude-v2\",\n",
    "              #context_size=512,\n",
    "              temperature=0,\n",
    "              additional_kwargs={'max_tokens_to_sample': 512,'top_k': 10})\n",
    "\n",
    "embed_model = BedrockEmbedding().from_credentials(\n",
    "    model_name='amazon.titan-embed-g1-text-02'\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embed_model, \n",
    "                                               chunk_size=512)\n",
    "chunk_overlap = 20\n",
    "chunk_size = 512\n",
    "service_context = ServiceContext.from_defaults(llm=llm, \n",
    "                                               embed_model=embed_model, \n",
    "                                               chunk_size=chunk_size,\n",
    "                                               chunk_overlap=chunk_overlap,\n",
    "                                            )\n",
    "set_global_service_context(service_context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_fn = lambda filename: {\"file_path\": filename, \"file_name\": filename.replace('data/', \"\").replace('.pdf', \"\")}\n",
    "\n",
    "# automatically sets the metadata of each document according to filename_fn\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", file_metadata=filename_fn\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b924b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#review metadata\n",
    "print(documents[50].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.vector_stores import (\n",
    "    OpensearchVectorStore,\n",
    "    OpensearchVectorClient,\n",
    ")\n",
    "from llama_index import VectorStoreIndex, StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc787db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "host = os.environ['OPENSEARCH_COLLECTION'] # OpenSearch endpoint, for example: my-test-domain.us-east-1.aoss.amazonaws.com\n",
    "service = 'aoss'\n",
    "region = 'us-east-1'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "endpoint = 'https://' + os.environ['OPENSEARCH_COLLECTION']\n",
    "print(f'endpoint: {endpoint}')\n",
    "index_name = \"rag-eval-v1\"\n",
    "# OpensearchVectorClient stores text in this field by default\n",
    "text_field = \"content\"\n",
    "# OpensearchVectorClient stores embeddings in this field by default\n",
    "embedding_field = \"embedding\"\n",
    "\n",
    "client = OpensearchVectorClient(\n",
    "    endpoint=endpoint,\n",
    "    index=index_name, \n",
    "    dim=1536, \n",
    "    embedding_field=embedding_field, \n",
    "    text_field=text_field,\n",
    "    http_auth=auth, \n",
    "    use_ssl=True, \n",
    "    verify_certs=True, \n",
    "    connection_class=RequestsHttpConnection, \n",
    "    timeout=10,\n",
    ")\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize vector store\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# initialize an index using our sample data and the client we just created\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ce1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run query\n",
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"Who is Amazon's Senior Vice President and General Counsel?\")\n",
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5b7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query with filtering - NOT WORKING ATM\n",
    "from llama_index import Document\n",
    "from llama_index.vector_stores.types import MetadataFilters, ExactMatchFilter, MetadataFilter,FilterOperator\n",
    "import regex as re\n",
    "\n",
    "# Create a query engine that only searches certain documents.\n",
    "metadata_query_engine = index.as_query_engine(\n",
    "    filters=MetadataFilters(\n",
    "        filters=[\n",
    "            ExactMatchFilter(\n",
    "                key=\"term\", value='{\"file_path\": \"data/AMZN-2023-10k.pdf\"}'\n",
    "            )\n",
    "            #ExactMatchFilter(key=\"file_name\", value=\"AMZN-2023-10k\")\n",
    "            \n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "res = metadata_query_engine.query(\n",
    "    \"who is Amazon's Senior Vice President and General Counsel?\"\n",
    ")\n",
    "res.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what the llm and embeddings model get to see\n",
    "from llama_index import Document\n",
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "document = documents[0]\n",
    "print(\n",
    "    \"The LLM sees this: \\n\",\n",
    "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
    ")\n",
    "print(\n",
    "    \"The Embedding model sees this: \\n\",\n",
    "    document.get_content(metadata_mode=MetadataMode.EMBED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c60cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Bedrock Knowledgebase retriever\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "kb_id = \"<knowledge_base_id>\"\n",
    "\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\",\n",
    "                              config=bedrock_config)\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "        knowledge_base_id=kb_id,\n",
    "        retrieval_config={\"vectorSearchConfiguration\": {\"numberOfResults\": 4}},\n",
    "\n",
    "    )\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": claude_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New service context for eval\n",
    "# good blog: https://levelup.gitconnected.com/evaluation-driven-development-the-swiss-army-knife-for-rag-pipelines-dba24218d47e\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "rag-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
